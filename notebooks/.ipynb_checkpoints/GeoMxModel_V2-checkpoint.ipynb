{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/users/nfs_a/aa16/.local/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import RangeIndex\n",
      "Can not use cuDNN on context None: Disabled by dnn.enabled flag\n",
      "Mapped name None to device cuda: Tesla V100-SXM2-32GB (0000:62:00.0)\n"
     ]
    }
   ],
   "source": [
    "import sys, ast, os\n",
    "import time\n",
    "import pickle\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from plotnine import *\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "data_type = 'float32'\n",
    "os.environ[\"THEANO_FLAGS\"] = 'device=cuda,floatX=' + data_type + ',force_device=True' + ',dnn.enabled=False'\n",
    "# /nfs/team283/vk7/software/miniconda3farm5/envs/cellpymc/bin/pip install git+https://github.com/vitkl/cell2location.git\n",
    "#sys.path.insert(1, '/nfs/team283/ed6/cellLocModel/')\n",
    "path = '/nfs/team283/aa16/'\n",
    "#import cellLocModel as clm\n",
    "import pycell2location.models as c2l\n",
    "import pycell2location.plt as c2lpl\n",
    "import pycell2location.cluster_averages \n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "# scanpy prints a lot of warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and metadata:\n",
    "\n",
    "counts = pd.read_table(path + 'KR_NAS/Sanger_288ROIs_TargetCountMatrix.txt')\n",
    "genes = counts['TargetName']\n",
    "counts = counts.drop('TargetName',1)\n",
    "counts = counts.rename(index=genes)\n",
    "counts = counts.astype('int')\n",
    "subset_genes = np.array([sum(counts.iloc[i,:]) for i in range(len(counts.iloc[:,0]))]) > 3 * 288\n",
    "counts = counts[subset_genes]\n",
    "metadata = pd.read_csv(path + 'KR_NAS/NanoString sequencing all annotations 2020.02.10.csv')\n",
    "metadata = metadata.iloc[0:286,]\n",
    "metadata = metadata.rename(index=metadata['Sanger_sampleID'])\n",
    "metadata = metadata.reindex(np.array(counts.columns))\n",
    "properties = pd.read_table(path + 'KR_NAS/Sanger_288ROIs_SegmentProperties.txt')\n",
    "properties = properties.rename(index=properties['DSP_Sample_ID'])\n",
    "properties = properties.reindex(np.array(metadata['Sample_ID']))\n",
    "columnNames = ('x', 'y', 'total_counts', 'Q3_counts')\n",
    "sample_info = pd.DataFrame(index=metadata['Sample_ID'], columns=columnNames)\n",
    "sample_info['x'] = np.array(metadata['VCDepth'])\n",
    "sample_info['y'] = np.array(metadata['Radial_position'])\n",
    "sample_info['total_counts'] = [sum(counts.iloc[:,i]) for i in range(len(counts.iloc[1,:]))] \n",
    "sample_info['Q3_counts'] = [sum(np.sort(counts.iloc[:,i])[int(np.round(0.5*len(counts.iloc[:,i]))):int(np.round(0.75*len(counts.iloc[:,i])))]) for i in range(len(counts.iloc[1,:]))]\n",
    "negProbes = pd.read_csv(path + 'KR_NAS/Sanger_dedup_withNegProbes.csv')\n",
    "negProbes = negProbes.loc[negProbes.iloc[:,0] == 'NegProbe-WTX',]\n",
    "genes = negProbes['Unnamed: 0']\n",
    "negProbes = negProbes.drop('Unnamed: 0',1)\n",
    "negProbes = negProbes.rename(index=genes)\n",
    "negProbes = negProbes.astype('int')\n",
    "negProbes = negProbes[counts.keys()]\n",
    "polioudakis = pd.read_csv('/nfs/team283/brainData/human_fetal/Polioudakis2019/cellStateMatrix.csv')\n",
    "genes = polioudakis.iloc[:,0]\n",
    "polioudakis = polioudakis.drop('Unnamed: 0',1)\n",
    "polioudakis = polioudakis.rename(index=genes)\n",
    "\n",
    "# Choose which ROIs and genes to use, e.g. top 100 markers of each cell type:\n",
    "rSlides = ('00MU', '00MV', '00MV-2')\n",
    "\n",
    "markers = pd.read_csv('/nfs/team283/brainData/human_fetal/Polioudakis2019/clusterMarkers.csv')\n",
    "\n",
    "# Choose the top N markers only:\n",
    "\n",
    "N = 75\n",
    "columnNames = np.unique(markers['cluster'])\n",
    "topN_markers = pd.DataFrame(index=range(N), columns=columnNames)\n",
    "for i in range(len(columnNames)):\n",
    "    topN_temp = np.array(markers[markers['cluster'] == columnNames[i]].iloc[0:N,].iloc[:,0])\n",
    "    topN_markers[columnNames[i]] = [topN_temp[i].split('.')[0] for i in range(len(topN_temp))]\n",
    "genes_new = np.array(topN_markers).flatten()  \n",
    "\n",
    "subset = np.where([metadata['slide'][i] in rSlides and metadata['Radial_position'][i] in (1,2,3)\n",
    "                   and metadata['AOI_type'][i] in ('Geometric', 'EOMESpos', 'HOPXpos', 'Ring', 'Residual')\n",
    "                            for i in range(len(metadata['slide']))])[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = np.array(negProbes.iloc[:,subset]).T\n",
    "counts_subset = counts.iloc[:,subset]\n",
    "subset_genes = [counts_subset.index[i] in genes_new for i in range(len(counts_subset.index))]\n",
    "counts_subset = counts_subset.iloc[subset_genes,:]\n",
    "cellstateMatrix = polioudakis.reindex(np.array(counts_subset.index))\n",
    "n_nuclei = np.array(properties['nuclei'])[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801, 112)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(counts_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"V7 Cell location model with better motivated hierarchical priors.\"\"\"\n",
    "\n",
    "import sys, ast, os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "import pickle\n",
    "import theano\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "from pycell2location.models.pymc3_loc_model import Pymc3LocModel \n",
    "\n",
    "# defining the model itself\n",
    "class LocationModelV7_V4_V4_GeoMx_V1(Pymc3LocModel):\n",
    "    r\"\"\"V7 Cell location model with better motivated hierarchical priors. \n",
    "    This model assumes the cell types present in each spot share an equal amount of total spot budget \n",
    "    (in terms of total N cells). \n",
    "    Relaxing that assumption would mean learning how large individual cell types \n",
    "    are in terms of their size / contribution to spots / extraction efficiencies.\n",
    "    :param cell_state_mat: Pandas data frame with gene signatures - genes in row, cell states or factors in columns\n",
    "    :param X_data: Numpy array of gene expression (cols) in spatial locations (rows)\n",
    "    :param learning_rate: ADAM learning rate for optimising Variational inference objective\n",
    "    :param n_iter: number of training iterations\n",
    "    :param total_grad_norm_constraint: gradient constraints in optimisation\n",
    "    :param gene_level_prior: prior on change in sensitivity between single cell and spatial (mean), \n",
    "                                how much it varies across cells (sd),\n",
    "                                and how certain we are in those numbers (mean_var_ratio) \n",
    "                                 - by default the variance in our prior of mean and sd is equal to the mean and sd\n",
    "                                 descreasing this number means having higher uncertainty about your prior\n",
    "    :param cell_number_prior: prior on cell density parameter:\n",
    "                                cells_per_spot - guess of the number of cells per location\n",
    "                                factors_per_spot - guess on the number of cell types \n",
    "                                                        / number of factors expressed per locations\n",
    "                                cells_mean_var_ratio, factors_mean_var_ratio - uncertainty in that guess\n",
    "                                                        measured as mean/var ratio, numbers < 1 mean high uncertainty\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cell_state_mat: np.ndarray,\n",
    "        X_data: np.ndarray,\n",
    "        data_type: str = 'float32',\n",
    "        n_iter = 15000,\n",
    "        learning_rate = 0.005,\n",
    "        total_grad_norm_constraint = 200,\n",
    "        verbose = True,\n",
    "        var_names=None, var_names_read=None,\n",
    "        obs_names=None, fact_names=None, sample_id=None,\n",
    "        gene_level_prior={'mean': 1/8, 'sd': 1/8, 'mean_var_ratio': 1},\n",
    "        cell_number_prior={'cells_per_spot': 7, 'factors_per_spot': 6,\n",
    "                           'cells_mean_var_ratio': 1, 'factors_mean_var_ratio': 1}\n",
    "    ):\n",
    "\n",
    "        ############# Initialise parameters ################\n",
    "        super().__init__(cell_state_mat, X_data,\n",
    "                         data_type, n_iter, \n",
    "                         learning_rate, total_grad_norm_constraint,\n",
    "                         verbose, var_names, var_names_read,\n",
    "                         obs_names, fact_names, sample_id)\n",
    "        \n",
    "        self.gene_level_prior = gene_level_prior\n",
    "        self.cell_number_prior = cell_number_prior        \n",
    "        \n",
    "        ############# Define the model ################\n",
    "        self.model = pm.Model()\n",
    "\n",
    "        with self.model:\n",
    "            \n",
    "            # =====================Gene expression level scaling======================= #\n",
    "            # Explains difference in expression between genes and \n",
    "            # how it differs in single cell and spatial technology\n",
    "            # compute hyperparameters from mean and sd\n",
    "            shape = gene_level_prior['mean']**2 / gene_level_prior['sd']**2\n",
    "            rate = gene_level_prior['mean'] / gene_level_prior['sd']**2 \n",
    "            shape_var = shape / gene_level_prior['mean_var_ratio']\n",
    "            rate_var = rate / gene_level_prior['mean_var_ratio']\n",
    "            self.gene_level_alpha_hyp = pm.Gamma('gene_level_alpha_hyp',\n",
    "                                                 mu=shape, sigma=np.sqrt(shape_var),\n",
    "                                                 shape=(1, 1))\n",
    "            self.gene_level_beta_hyp = pm.Gamma('gene_level_beta_hyp', \n",
    "                                                 mu=rate, sigma=np.sqrt(rate_var),\n",
    "                                                 shape=(1, 1))\n",
    "        \n",
    "            self.gene_level = pm.Gamma('gene_level', self.gene_level_alpha_hyp,\n",
    "                                       self.gene_level_beta_hyp, shape=(self.n_genes, 1))\n",
    "        \n",
    "            # scale cell state factors by gene_level\n",
    "            self.gene_factors = pm.Deterministic('gene_factors', self.cell_state)\n",
    "            #tt.printing.Print('gene_factors sum')(gene_factors.sum(0).shape)\n",
    "            #tt.printing.Print('gene_factors sum')(gene_factors.sum(0))\n",
    "    \n",
    "            # =====================Spot factors======================= #\n",
    "            # prior on spot factors reflects the number of cells, fraction of their cytoplasm captured, \n",
    "            # times heterogeniety in the total number of mRNA between individual cells with each cell type\n",
    "            self.cells_per_spot = pm.Gamma('cells_per_spot',\n",
    "                                           mu=self.cell_number_prior['cells_per_spot'],\n",
    "                                           sigma=np.sqrt(self.cell_number_prior['cells_per_spot'] \\\n",
    "                                                         / self.cell_number_prior['cells_mean_var_ratio']),\n",
    "                                           shape=(self.n_cells, 1))\n",
    "            self.factors_per_spot = pm.Gamma('factors_per_spot', \n",
    "                                             mu=self.cell_number_prior['factors_per_spot'], \n",
    "                                             sigma=np.sqrt(self.cell_number_prior['factors_per_spot'] \\\n",
    "                                                           / self.cell_number_prior['factors_mean_var_ratio']),\n",
    "                                             shape=(self.n_cells, 1))\n",
    "            \n",
    "            shape = self.factors_per_spot / np.array(self.n_fact).reshape((1, 1))\n",
    "            rate = tt.ones((1, 1)) / self.cells_per_spot * self.factors_per_spot\n",
    "            self.spot_factors = pm.Gamma('spot_factors', alpha=shape, beta=rate,\n",
    "                                         shape=(self.n_cells, self.n_fact))\n",
    "    \n",
    "            # =====================Spot-specific additive component======================= #\n",
    "            # molecule contribution that cannot be explained by cell state signatures\n",
    "            # these counts are distributed between all genes not just expressed genes\n",
    "            self.spot_add_hyp = pm.Gamma('spot_add_hyp', 1, 0.1, shape=2)\n",
    "            self.spot_add = pm.Gamma('spot_add', self.spot_add_hyp[0],\n",
    "                                     self.spot_add_hyp[1], shape=(self.n_cells, 1))\n",
    "            \n",
    "            # =====================Gene-specific additive component ======================= #\n",
    "            # per gene molecule contribution that cannot be explained by cell state signatures\n",
    "            # these counts are distributed equally between all spots (e.g. background, free-floating RNA)\n",
    "            self.gene_add_hyp = pm.Gamma('gene_add_hyp', 1, 1, shape=2)\n",
    "            self.gene_add = pm.Gamma('gene_add', self.gene_add_hyp[0],\n",
    "                                     self.gene_add_hyp[1], shape=(self.n_genes, 1))\n",
    "    \n",
    "            # =====================Expected expression ======================= #\n",
    "            # expected expression\n",
    "            self.mu_biol = pm.math.dot(self.spot_factors, self.gene_factors.T) * self.gene_level.T + self.gene_add.T + self.spot_add\n",
    "            #tt.printing.Print('mu_biol')(self.mu_biol.shape)\n",
    "    \n",
    "            # =====================DATA likelihood ======================= #\n",
    "            # Likelihood (sampling distribution) of observations & add overdispersion via NegativeBinomial / Poisson\n",
    "            self.data_target = pm.Poisson('data_target', mu=self.mu_biol,\n",
    "                                          observed=self.x_data,\n",
    "                                          total_size=self.X_data.shape)\n",
    "                                          \n",
    "            # =====================Compute nUMI from each factor in spots  ======================= #                          \n",
    "            self.nUMI_factors = pm.Deterministic('nUMI_factors',\n",
    "                                                 (self.spot_factors * (self.gene_factors * self.gene_level).sum(0)))\n",
    "    \n",
    "    def plot_posterior_vs_dataV1(self):\n",
    "        self.plot_posterior_vs_data(gene_fact_name = 'gene_factors',\n",
    "                               cell_fact_name = 'spot_factors_scaled')\n",
    "    \n",
    "    def plot_biol_spot_nUMI(self, fact_name='nUMI_factors'):\n",
    "        \n",
    "        plt.hist(np.log10(self.samples['post_sample_means'][fact_name].sum(1)), bins = 50)\n",
    "        plt.xlabel('Biological spot nUMI (log10)')\n",
    "        plt.title('Biological spot nUMI')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def plot_spot_add(self):\n",
    "        \n",
    "        plt.hist(np.log10(self.samples['post_sample_means']['spot_add'][:,0]), bins = 50)\n",
    "        plt.xlabel('UMI unexplained by biological factors')\n",
    "        plt.title('Additive technical spot nUMI')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        \n",
    "    def plot_gene_add(self):\n",
    "        \n",
    "        plt.hist((self.samples['post_sample_means']['gene_add'][:,0]), bins = 50)\n",
    "        plt.xlabel('S_g additive background noise parameter')\n",
    "        plt.title('S_g additive background noise parameter')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def plot_gene_level(self):\n",
    "        \n",
    "        plt.hist((self.samples['post_sample_means']['gene_level'][:,0]), bins = 50)\n",
    "        plt.xlabel('M_g expression level scaling parameter')\n",
    "        plt.title('M_g expression level scaling parameter')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def compute_expected(self):\n",
    "        r\"\"\" Compute expected expression of each gene in each spot (Poisson mu). Useful for evaluating how well the model learned expression pattern of all genes in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute the poisson rate\n",
    "        self.mu = np.dot(self.samples['post_sample_means']['spot_factors'],\n",
    "                         self.samples['post_sample_means']['gene_factors'].T) \\\n",
    "        * self.samples['post_sample_means']['gene_level'].T \\\n",
    "        + self.samples['post_sample_means']['gene_add'].T \\\n",
    "        + self.samples['post_sample_means']['spot_add']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"V7 Cell location model with better motivated hierarchical priors.\"\"\"\n",
    "\n",
    "import sys, ast, os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "import pickle\n",
    "import theano\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "from pycell2location.models.pymc3_loc_model import Pymc3LocModel \n",
    "\n",
    "# defining the model itself\n",
    "class LocationModelV7_V4_V4_GeoMx_V2(Pymc3LocModel):\n",
    "    r\"\"\"V7 Cell location model with better motivated hierarchical priors. Additionally, uses Gamma and not Poisson\n",
    "    for counts over 10000. \n",
    "    This model assumes the cell types present in each spot share an equal amount of total spot budget \n",
    "    (in terms of total N cells). \n",
    "    Relaxing that assumption would mean learning how large individual cell types \n",
    "    are in terms of their size / contribution to spots / extraction efficiencies.\n",
    "    :param cell_state_mat: Pandas data frame with gene signatures - genes in row, cell states or factors in columns\n",
    "    :param X_data: Numpy array of gene expression (cols) in spatial locations (rows)\n",
    "    :param learning_rate: ADAM learning rate for optimising Variational inference objective\n",
    "    :param n_iter: number of training iterations\n",
    "    :param total_grad_norm_constraint: gradient constraints in optimisation\n",
    "    :param gene_level_prior: prior on change in sensitivity between single cell and spatial (mean), \n",
    "                                how much it varies across cells (sd),\n",
    "                                and how certain we are in those numbers (mean_var_ratio) \n",
    "                                 - by default the variance in our prior of mean and sd is equal to the mean and sd\n",
    "                                 descreasing this number means having higher uncertainty about your prior\n",
    "    :param cell_number_prior: prior on cell density parameter:\n",
    "                                cells_per_spot - guess of the number of cells per location\n",
    "                                factors_per_spot - guess on the number of cell types \n",
    "                                                        / number of factors expressed per locations\n",
    "                                cells_mean_var_ratio, factors_mean_var_ratio - uncertainty in that guess\n",
    "                                                        measured as mean/var ratio, numbers < 1 mean high uncertainty\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cell_state_mat: np.ndarray,\n",
    "        X_data: np.ndarray,\n",
    "        data_type: str = 'float32',\n",
    "        n_iter = 15000,\n",
    "        learning_rate = 0.005,\n",
    "        total_grad_norm_constraint = 200,\n",
    "        verbose = True,\n",
    "        var_names=None, var_names_read=None,\n",
    "        obs_names=None, fact_names=None, sample_id=None,\n",
    "        gene_level_prior={'mean': 1/8, 'sd': 1/8, 'mean_var_ratio': 1},\n",
    "        cell_number_prior={'cells_per_spot': 7, 'factors_per_spot': 6,\n",
    "                           'cells_mean_var_ratio': 1, 'factors_mean_var_ratio': 1},\n",
    "        cutoff_poisson = 10000\n",
    "    ):\n",
    "\n",
    "        ############# Initialise parameters ################\n",
    "        super().__init__(cell_state_mat, X_data,\n",
    "                         data_type, n_iter, \n",
    "                         learning_rate, total_grad_norm_constraint,\n",
    "                         verbose, var_names, var_names_read,\n",
    "                         obs_names, fact_names, sample_id)\n",
    "        \n",
    "        self.gene_level_prior = gene_level_prior\n",
    "        self.cell_number_prior = cell_number_prior\n",
    "        self.cutoff_poisson = cutoff_poisson\n",
    "        self.poisson_residual = self.X_data < self.cutoff_poisson\n",
    "        self.gamma_residual = self.X_data > self.cutoff_poisson\n",
    "        self.X_data1 = self.X_data[self.poisson_residual]\n",
    "        self.X_data2 = self.X_data[self.gamma_residual]\n",
    "        \n",
    "        ############# Define the model ################\n",
    "        self.model = pm.Model()\n",
    "\n",
    "        with self.model:\n",
    "            \n",
    "            # =====================Gene expression level scaling======================= #\n",
    "            # Explains difference in expression between genes and \n",
    "            # how it differs in single cell and spatial technology\n",
    "            # compute hyperparameters from mean and sd\n",
    "            shape = gene_level_prior['mean']**2 / gene_level_prior['sd']**2\n",
    "            rate = gene_level_prior['mean'] / gene_level_prior['sd']**2 \n",
    "            shape_var = shape / gene_level_prior['mean_var_ratio']\n",
    "            rate_var = rate / gene_level_prior['mean_var_ratio']\n",
    "            self.gene_level_alpha_hyp = pm.Gamma('gene_level_alpha_hyp',\n",
    "                                                 mu=shape, sigma=np.sqrt(shape_var),\n",
    "                                                 shape=(1, 1))\n",
    "            self.gene_level_beta_hyp = pm.Gamma('gene_level_beta_hyp', \n",
    "                                                 mu=rate, sigma=np.sqrt(rate_var),\n",
    "                                                 shape=(1, 1))\n",
    "        \n",
    "            self.gene_level = pm.Gamma('gene_level', self.gene_level_alpha_hyp,\n",
    "                                       self.gene_level_beta_hyp, shape=(self.n_genes, 1))\n",
    "        \n",
    "            # scale cell state factors by gene_level\n",
    "            self.gene_factors = pm.Deterministic('gene_factors', self.cell_state)\n",
    "            #tt.printing.Print('gene_factors sum')(gene_factors.sum(0).shape)\n",
    "            #tt.printing.Print('gene_factors sum')(gene_factors.sum(0))\n",
    "    \n",
    "            # =====================Spot factors======================= #\n",
    "            # prior on spot factors reflects the number of cells, fraction of their cytoplasm captured, \n",
    "            # times heterogeniety in the total number of mRNA between individual cells with each cell type\n",
    "            self.cells_per_spot = pm.Gamma('cells_per_spot',\n",
    "                                           mu=self.cell_number_prior['cells_per_spot'],\n",
    "                                           sigma=np.sqrt(self.cell_number_prior['cells_per_spot'] \\\n",
    "                                                         / self.cell_number_prior['cells_mean_var_ratio']),\n",
    "                                           shape=(self.n_cells, 1))\n",
    "            self.factors_per_spot = pm.Gamma('factors_per_spot', \n",
    "                                             mu=self.cell_number_prior['factors_per_spot'], \n",
    "                                             sigma=np.sqrt(self.cell_number_prior['factors_per_spot'] \\\n",
    "                                                           / self.cell_number_prior['factors_mean_var_ratio']),\n",
    "                                             shape=(self.n_cells, 1))\n",
    "            \n",
    "            shape = self.factors_per_spot / np.array(self.n_fact).reshape((1, 1))\n",
    "            rate = tt.ones((1, 1)) / self.cells_per_spot * self.factors_per_spot\n",
    "            self.spot_factors = pm.Gamma('spot_factors', alpha=shape, beta=rate,\n",
    "                                         shape=(self.n_cells, self.n_fact))\n",
    "    \n",
    "            # =====================Spot-specific additive component======================= #\n",
    "            # molecule contribution that cannot be explained by cell state signatures\n",
    "            # these counts are distributed between all genes not just expressed genes\n",
    "            self.spot_add_hyp = pm.Gamma('spot_add_hyp', 1, 0.1, shape=2)\n",
    "            self.spot_add = pm.Gamma('spot_add', self.spot_add_hyp[0],\n",
    "                                     self.spot_add_hyp[1], shape=(self.n_cells, 1))\n",
    "            \n",
    "            # =====================Gene-specific additive component ======================= #\n",
    "            # per gene molecule contribution that cannot be explained by cell state signatures\n",
    "            # these counts are distributed equally between all spots (e.g. background, free-floating RNA)\n",
    "            self.gene_add_hyp = pm.Gamma('gene_add_hyp', 1, 1, shape=2)\n",
    "            self.gene_add = pm.Gamma('gene_add', self.gene_add_hyp[0],\n",
    "                                     self.gene_add_hyp[1], shape=(self.n_genes, 1))\n",
    "    \n",
    "            # =====================Expected expression ======================= #\n",
    "            # expected expression\n",
    "            self.mu_biol = pm.math.dot(self.spot_factors, self.gene_factors.T) * self.gene_level.T + self.gene_add.T + self.spot_add          \n",
    "            \n",
    "            # =====================DATA likelihood ======================= #\n",
    "            # Likelihood (sampling distribution) of observations & add overdispersion via NegativeBinomial / Poisson\n",
    "            self.data_target1 = pm.Poisson('data_target1', mu=self.mu_biol[self.poisson_residual],\n",
    "                                          observed=self.x_data[self.poisson_residual],\n",
    "                                          total_size=self.X_data[self.poisson_residual].shape)\n",
    "            \n",
    "            self.data_target2 = pm.Gamma('data_target2', mu=self.mu_biol[self.gamma_residual],\n",
    "                                          sd = np.sqrt(self.mu_biol[self.gamma_residual]),\n",
    "                                          observed=self.x_data[self.gamma_residual],\n",
    "                                          total_size=self.X_data[self.gamma_residual].shape)\n",
    "                                          \n",
    "            # =====================Compute nUMI from each factor in spots  ======================= #                          \n",
    "            self.nUMI_factors = pm.Deterministic('nUMI_factors',\n",
    "                                                 (self.spot_factors * (self.gene_factors * self.gene_level).sum(0)))\n",
    "    \n",
    "    def plot_posterior_vs_dataV1(self):\n",
    "        self.plot_posterior_vs_data(gene_fact_name = 'gene_factors',\n",
    "                               cell_fact_name = 'spot_factors_scaled')\n",
    "    \n",
    "    def plot_biol_spot_nUMI(self, fact_name='nUMI_factors'):\n",
    "        \n",
    "        plt.hist(np.log10(self.samples['post_sample_means'][fact_name].sum(1)), bins = 50)\n",
    "        plt.xlabel('Biological spot nUMI (log10)')\n",
    "        plt.title('Biological spot nUMI')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def plot_spot_add(self):\n",
    "        \n",
    "        plt.hist(np.log10(self.samples['post_sample_means']['spot_add'][:,0]), bins = 50)\n",
    "        plt.xlabel('UMI unexplained by biological factors')\n",
    "        plt.title('Additive technical spot nUMI')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        \n",
    "    def plot_gene_add(self):\n",
    "        \n",
    "        plt.hist((self.samples['post_sample_means']['gene_add'][:,0]), bins = 50)\n",
    "        plt.xlabel('S_g additive background noise parameter')\n",
    "        plt.title('S_g additive background noise parameter')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def plot_gene_level(self):\n",
    "        \n",
    "        plt.hist((self.samples['post_sample_means']['gene_level'][:,0]), bins = 50)\n",
    "        plt.xlabel('M_g expression level scaling parameter')\n",
    "        plt.title('M_g expression level scaling parameter')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def compute_expected(self):\n",
    "        r\"\"\" Compute expected expression of each gene in each spot (Poisson mu). Useful for evaluating how well the model learned expression pattern of all genes in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute the poisson rate/gamma mean:\n",
    "        self.mu = np.dot(self.samples['post_sample_means']['spot_factors'],\n",
    "                         self.samples['post_sample_means']['gene_factors'].T) \\\n",
    "        * self.samples['post_sample_means']['gene_level'].T \\\n",
    "        + self.samples['post_sample_means']['gene_add'].T \\\n",
    "        + self.samples['post_sample_means']['spot_add']\n",
    "        \n",
    "        self.mu1 = self.mu[self.poisson_residual]\n",
    "        self.mu2 = self.mu[self.gamma_residual] \n",
    "        \n",
    "    def plot_prior_vs_data(self, data_target_name=('data_target1', 'data_target2'),\n",
    "                           data_node=('X_data1', 'X_data2')):\n",
    "        r\"\"\" Plot data vs a single sample from the prior in a 2D histogram\n",
    "        Uses self.X_data and self.prior_trace['data_target'].\n",
    "        :param data_node: name of the object slot containing data\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(data_node) == 1:\n",
    "            data_node = getattr(self, data_node)\n",
    "            plt.hist2d(np.log10(data_node.flatten()+1),\n",
    "            np.log10(self.prior_trace[data_target_name].flatten()+1),\n",
    "            bins = 50, norm=matplotlib.colors.LogNorm())\n",
    "            plt.xlabel('Data, log10(nUMI)')\n",
    "            plt.ylabel('Prior sample, log10(nUMI)')\n",
    "            plt.title('UMI counts (all spots, all genes)')\n",
    "            plt.tight_layout()\n",
    "        else:\n",
    "            data_node_list = []\n",
    "            for i in range(len(data_node)):\n",
    "                data_node_list.append(getattr(self, data_node[i]))\n",
    "            fig, ax = plt.subplots(1, 2, figsize = (20,10))\n",
    "            for i in range(len(data_node_list)):\n",
    "                ax[i].hist2d(np.log10(data_node_list[i].flatten()+1),\n",
    "                np.log10(self.prior_trace[data_target_name[i]].flatten()+1),\n",
    "                bins = 50, norm=matplotlib.colors.LogNorm())\n",
    "                ax[i].set_xlabel('Data, log10(nUMI)', fontsize=25)\n",
    "                ax[i].set_ylabel('Prior sample, log10(nUMI)', fontsize=25)\n",
    "                ax[i].set_title(data_target_name[i] + ': UMI counts (all spots, all genes)', fontsize=25)\n",
    "                ax[i].tick_params(axis='both', which='major', labelsize=25)\n",
    "                ax[i].tick_params(axis='both', which='minor', labelsize=25)\n",
    "\n",
    "            plt.tight_layout()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"V7 Cell location model with better motivated hierarchical priors.\"\"\"\n",
    "\n",
    "import sys, ast, os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "import pickle\n",
    "import theano\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pycell2location.models.pymc3_loc_model import Pymc3LocModel \n",
    "\n",
    "# defining the model itself\n",
    "class LocationModelV7_V4_V4_GeoMx_V3(Pymc3LocModel):\n",
    "    r\"\"\"V7 Cell location model with better motivated hierarchical priors. Additionally, uses Gamma and not Poisson\n",
    "    for counts over 10000. Additionally, uses a more accurate model for the background counts, based on negative probe counts,\n",
    "    in which background increases linearly with the total number of counts in each spot.\n",
    "    This model assumes the cell types present in each spot share an equal amount of total spot budget \n",
    "    (in terms of total N cells). \n",
    "    Relaxing that assumption would mean learning how large individual cell types \n",
    "    are in terms of their size / contribution to spots / extraction efficiencies.\n",
    "    :param cell_state_mat: Pandas data frame with gene signatures - genes in row, cell states or factors in columns\n",
    "    :param X_data: Numpy array of gene expression (cols) in spatial locations (rows)\n",
    "    :param learning_rate: ADAM learning rate for optimising Variational inference objective\n",
    "    :param n_iter: number of training iterations\n",
    "    :param total_grad_norm_constraint: gradient constraints in optimisation\n",
    "    :param gene_level_prior: prior on change in sensitivity between single cell and spatial (mean), \n",
    "                                how much it varies across cells (sd),\n",
    "                                and how certain we are in those numbers (mean_var_ratio) \n",
    "                                 - by default the variance in our prior of mean and sd is equal to the mean and sd\n",
    "                                 descreasing this number means having higher uncertainty about your prior\n",
    "    :param cell_number_prior: prior on cell density parameter:\n",
    "                                cells_per_spot - guess of the number of cells per location\n",
    "                                factors_per_spot - guess on the number of cell types \n",
    "                                                        / number of factors expressed per locations\n",
    "                                cells_mean_var_ratio, factors_mean_var_ratio - uncertainty in that guess\n",
    "                                                        measured as mean/var ratio, numbers < 1 mean high uncertainty\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cell_state_mat: np.ndarray,\n",
    "        X_data: np.ndarray,\n",
    "        Y_data: np.ndarray,\n",
    "        data_type: str = 'float32',\n",
    "        n_iter = 15000,\n",
    "        learning_rate = 0.005,\n",
    "        total_grad_norm_constraint = 200,\n",
    "        verbose = True,\n",
    "        var_names=None, var_names_read=None,\n",
    "        obs_names=None, fact_names=None, sample_id=None,\n",
    "        gene_level_prior={'mean': 1/8, 'sd': 1/8, 'mean_var_ratio': 1},\n",
    "        cell_number_prior={'cells_per_spot': 7, 'factors_per_spot': 6,\n",
    "                           'cells_mean_var_ratio': 1, 'factors_mean_var_ratio': 1},\n",
    "        cutoff_poisson = 10000,\n",
    "        cv_y_rn = 0.01\n",
    "    ):\n",
    "\n",
    "        ############# Initialise parameters ################\n",
    "        super().__init__(cell_state_mat, X_data,\n",
    "                         data_type, n_iter, \n",
    "                         learning_rate, total_grad_norm_constraint,\n",
    "                         verbose, var_names, var_names_read,\n",
    "                         obs_names, fact_names, sample_id)\n",
    "        \n",
    "        self.gene_level_prior = gene_level_prior\n",
    "        self.cell_number_prior = cell_number_prior\n",
    "        self.cutoff_poisson = cutoff_poisson\n",
    "        self.poisson_residual = self.X_data < self.cutoff_poisson\n",
    "        self.gamma_residual = self.X_data > self.cutoff_poisson\n",
    "        self.X_data1 = self.X_data[self.poisson_residual]\n",
    "        self.X_data2 = self.X_data[self.gamma_residual]\n",
    "        \n",
    "        self.Y_data = Y_data\n",
    "        self.y_data = theano.shared(Y_data.astype(self.data_type))\n",
    "        self.n_rois = Y_data.shape[0]\n",
    "        self.x_r = np.array([np.sum(X_data[i,:]) for i in range(self.n_rois)]).reshape(self.n_rois,1)/10**(5)\n",
    "        self.cv_y_r = cv_y_rn\n",
    "        self.n_npro = Y_data.shape[1]\n",
    "        \n",
    "        ############# Define the model ################\n",
    "        self.model = pm.Model()\n",
    "\n",
    "        with self.model:\n",
    "            # =====================Negative Probe Counts======================= #\n",
    "            # Explain negative probe counts as linearly dependent on total counts and with\n",
    "            # scaling coefficient a_n drawn from a gamma distribution.\n",
    "            \n",
    "            # Prior for distribution of negative probe count levels:\n",
    "            self.a_n_hyper = pm.Gamma('a_n_hyper', mu = np.array((2,1)), sd = np.array((0.1,0.1)), shape = 2)\n",
    "            # The distribution of negative probe count levels:\n",
    "            self.a_n = pm.Gamma('a_n', alpha = self.a_n_hyper[0], beta = self.a_n_hyper[1], shape = (1,self.n_npro))\n",
    "            # The expected mean of each negative probe based on its count level and total counts in this roi:\n",
    "            self.mu_n = self.a_n*self.x_r\n",
    "            # How much deviation from this expected mean is allowed depends on the self.cv_y_r parameter:\n",
    "            self.y_rn = pm.Gamma('y_rn', mu = self.mu_n, sigma = self.cv_y_r*self.mu_n, shape = (self.n_rois,self.n_npro))\n",
    "            \n",
    "            # =====================Gene expression level scaling======================= #\n",
    "            # Explains difference in expression between genes and \n",
    "            # how it differs in single cell and spatial technology\n",
    "            # compute hyperparameters from mean and sd\n",
    "            shape = gene_level_prior['mean']**2 / gene_level_prior['sd']**2\n",
    "            rate = gene_level_prior['mean'] / gene_level_prior['sd']**2 \n",
    "            shape_var = shape / gene_level_prior['mean_var_ratio']\n",
    "            rate_var = rate / gene_level_prior['mean_var_ratio']\n",
    "            self.gene_level_alpha_hyp = pm.Gamma('gene_level_alpha_hyp',\n",
    "                                                 mu=shape, sigma=np.sqrt(shape_var),\n",
    "                                                 shape=(1, 1))\n",
    "            self.gene_level_beta_hyp = pm.Gamma('gene_level_beta_hyp', \n",
    "                                                 mu=rate, sigma=np.sqrt(rate_var),\n",
    "                                                 shape=(1, 1))\n",
    "        \n",
    "            self.gene_level = pm.Gamma('gene_level', self.gene_level_alpha_hyp,\n",
    "                                       self.gene_level_beta_hyp, shape=(self.n_genes, 1))\n",
    "        \n",
    "            # scale cell state factors by gene_level\n",
    "            self.gene_factors = pm.Deterministic('gene_factors', self.cell_state)\n",
    "    \n",
    "            # =====================Spot factors======================= #\n",
    "            # prior on spot factors reflects the number of cells, fraction of their cytoplasm captured, \n",
    "            # times heterogeniety in the total number of mRNA between individual cells with each cell type\n",
    "            self.cells_per_spot = pm.Gamma('cells_per_spot',\n",
    "                                           mu=self.cell_number_prior['cells_per_spot'],\n",
    "                                           sigma=np.sqrt(self.cell_number_prior['cells_per_spot'] \\\n",
    "                                                         / self.cell_number_prior['cells_mean_var_ratio']),\n",
    "                                           shape=(self.n_cells, 1))\n",
    "            self.factors_per_spot = pm.Gamma('factors_per_spot', \n",
    "                                             mu=self.cell_number_prior['factors_per_spot'], \n",
    "                                             sigma=np.sqrt(self.cell_number_prior['factors_per_spot'] \\\n",
    "                                                           / self.cell_number_prior['factors_mean_var_ratio']),\n",
    "                                             shape=(self.n_cells, 1))\n",
    "            \n",
    "            shape = self.factors_per_spot / np.array(self.n_fact).reshape((1, 1))\n",
    "            rate = tt.ones((1, 1)) / self.cells_per_spot * self.factors_per_spot\n",
    "            self.spot_factors = pm.Gamma('spot_factors', alpha=shape, beta=rate,\n",
    "                                         shape=(self.n_cells, self.n_fact))\n",
    "    \n",
    "            # =====================Spot-specific additive component======================= #\n",
    "            # molecule contribution that cannot be explained by cell state signatures\n",
    "            # these counts are distributed between all genes not just expressed genes\n",
    "            self.spot_add_hyp = pm.Gamma('spot_add_hyp', 1, 0.1, shape=2)\n",
    "            self.spot_add = pm.Gamma('spot_add', self.spot_add_hyp[0],\n",
    "                                     self.spot_add_hyp[1], shape=(self.n_cells, 1))\n",
    "            \n",
    "            # =====================Gene-specific additive component ======================= #\n",
    "            # This term models background and non-specific binding of probes.\n",
    "            # We assume it has is has the same distribution as negative probes:\n",
    "            self.a_g = pm.Gamma('a_g', alpha = self.a_n_hyper[0], beta = self.a_n_hyper[1], shape = (1, self.n_genes))\n",
    "            self.mu_gr = (self.a_g*self.x_r).T\n",
    "            self.gene_add = pm.Gamma('gene_add', mu = self.mu_gr, sigma = self.cv_y_r*self.mu_gr, shape = (self.n_genes, self.n_cells))\n",
    "    \n",
    "            # =====================Expected expression ======================= #\n",
    "            # expected expression\n",
    "            self.mu_biol = pm.math.dot(self.spot_factors, self.gene_factors.T) * self.gene_level.T + self.gene_add.T + self.spot_add\n",
    "            #tt.printing.Print('mu_biol')(self.mu_biol.shape)            \n",
    "            \n",
    "            # =====================DATA likelihood ======================= #\n",
    "            \n",
    "            # Poisson sampling of negative probes:\n",
    "            self.data_target0 = pm.Poisson('data_target0', mu=self.y_rn,\n",
    "                                          observed=self.y_data,\n",
    "                                          total_size=self.Y_data.shape)\n",
    "            \n",
    "            # Poisson sampling of gene counts up to 10000:\n",
    "            self.data_target1 = pm.Poisson('data_target1', mu=self.mu_biol[self.poisson_residual],\n",
    "                                          observed=self.x_data[self.poisson_residual],\n",
    "                                          total_size=self.X_data[self.poisson_residual].shape)\n",
    "            \n",
    "            # Approximate Poisson by Gamma for gene counts over 10000:\n",
    "            self.data_target2 = pm.Gamma('data_target2', mu=self.mu_biol[self.gamma_residual],\n",
    "                                          sd = np.sqrt(self.mu_biol[self.gamma_residual]),\n",
    "                                          observed=self.x_data[self.gamma_residual],\n",
    "                                          total_size=self.X_data[self.gamma_residual].shape)\n",
    "                                          \n",
    "            # =====================Compute nUMI from each factor in spots  ======================= #                          \n",
    "            self.nUMI_factors = pm.Deterministic('nUMI_factors',\n",
    "                                                 (self.spot_factors * (self.gene_factors * self.gene_level).sum(0)))\n",
    "    \n",
    "    def plot_posterior_vs_dataV1(self):\n",
    "        self.plot_posterior_vs_data(gene_fact_name = 'gene_factors',\n",
    "                               cell_fact_name = 'spot_factors_scaled')\n",
    "    \n",
    "    def plot_biol_spot_nUMI(self, fact_name='nUMI_factors'):\n",
    "        \n",
    "        plt.hist(np.log10(self.samples['post_sample_means'][fact_name].sum(1)), bins = 50)\n",
    "        plt.xlabel('Biological spot nUMI (log10)')\n",
    "        plt.title('Biological spot nUMI')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def plot_spot_add(self):\n",
    "        \n",
    "        plt.hist(np.log10(self.samples['post_sample_means']['spot_add'][:,0]), bins = 50)\n",
    "        plt.xlabel('UMI unexplained by biological factors')\n",
    "        plt.title('Additive technical spot nUMI')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        \n",
    "    def plot_gene_add(self):\n",
    "        \n",
    "        plt.hist((self.samples['post_sample_means']['gene_add'][:,0]), bins = 50)\n",
    "        plt.xlabel('S_g additive background noise parameter')\n",
    "        plt.title('S_g additive background noise parameter')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def plot_gene_level(self):\n",
    "        \n",
    "        plt.hist((self.samples['post_sample_means']['gene_level'][:,0]), bins = 50)\n",
    "        plt.xlabel('M_g expression level scaling parameter')\n",
    "        plt.title('M_g expression level scaling parameter')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def compute_expected(self):\n",
    "        r\"\"\" Compute expected expression of each gene in each spot (Poisson mu). Useful for evaluating how well the model learned expression pattern of all genes in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute the poisson rate/gamma mean:\n",
    "        self.mu = np.dot(self.samples['post_sample_means']['spot_factors'],\n",
    "                         self.samples['post_sample_means']['gene_factors'].T) \\\n",
    "        * self.samples['post_sample_means']['gene_level'].T \\\n",
    "        + self.samples['post_sample_means']['gene_add'].T \\\n",
    "        + self.samples['post_sample_means']['spot_add']\n",
    "        \n",
    "        self.mun = self.samples['post_sample_means']['a_n']*self.x_r\n",
    "        self.mu1 = self.mu[self.poisson_residual]\n",
    "        self.mu2 = self.mu[self.gamma_residual]\n",
    "        \n",
    "    def plot_prior_vs_data(self, data_target_name=('data_target0', 'data_target1', 'data_target2'),\n",
    "                           data_node=('Y_data', 'X_data1', 'X_data2')):\n",
    "        r\"\"\" Plot data vs a single sample from the prior in a 2D histogram\n",
    "        Uses self.X_data and self.prior_trace['data_target'].\n",
    "        :param data_node: name of the object slot containing data\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(data_node) == 1:\n",
    "            data_node = getattr(self, data_node)\n",
    "            plt.hist2d(np.log10(data_node.flatten()+1),\n",
    "            np.log10(self.prior_trace[data_target_name].flatten()+1),\n",
    "            bins = 50, norm=matplotlib.colors.LogNorm())\n",
    "            plt.xlabel('Data, log10(nUMI)')\n",
    "            plt.ylabel('Prior sample, log10(nUMI)')\n",
    "            plt.title('UMI counts (all spots, all genes)')\n",
    "            plt.tight_layout()\n",
    "        else:\n",
    "            data_node_list = []\n",
    "            for i in range(len(data_node)):\n",
    "                data_node_list.append(getattr(self, data_node[i]))\n",
    "            fig, ax = plt.subplots(1, 3, figsize = (30,10))\n",
    "            for i in range(len(data_node_list)):\n",
    "                ax[i].hist2d(np.log10(data_node_list[i].flatten()+1),\n",
    "                np.log10(self.prior_trace[data_target_name[i]].flatten()+1),\n",
    "                bins = 50, norm=matplotlib.colors.LogNorm())\n",
    "                ax[i].set_xlabel('Data, log10(nUMI)', fontsize=25)\n",
    "                ax[i].set_ylabel('Prior sample, log10(nUMI)', fontsize=25)\n",
    "                ax[i].set_title(data_target_name[i] + ': UMI counts (all spots, all genes)', fontsize=25)\n",
    "                ax[i].tick_params(axis='both', which='major', labelsize=25)\n",
    "                ax[i].tick_params(axis='both', which='minor', labelsize=25)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            \n",
    "    def sample_a_n_prior(self, n_samples = 100):\n",
    "        r\"\"\" Sample the a_n parameter prior.\n",
    "        :return: self.a_n_prior with 100 sampled values for a_n \n",
    "        \"\"\"\n",
    "        # Take multiple prior samples for certain parameters\n",
    "        self.a_n_prior = []\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            with self.model:\n",
    "                prior_trace = pm.sample_prior_predictive(samples=1, var_names = ['a_n'])\n",
    "            self.a_n_prior.append(prior_trace['a_n'][0][0])\n",
    "        \n",
    "    def plot_a_n_prior_vs_data(self, n_samples = 100):\n",
    "        r\"\"\" Sample the \n",
    "        :return: self.a_n_prior with 100 sampled values for a_n \n",
    "        \"\"\"\n",
    "        plt.hist(mod3.a_n_prior, bins = 50)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod1 = LocationModelV7_V4_V4_GeoMx_V1(\n",
    "#         np.array(cellstateMatrix), np.array(counts_subset).T,\n",
    "#         data_type='float32', n_iter=2000,\n",
    "#         learning_rate=0.001,\n",
    "#         total_grad_norm_constraint=200,\n",
    "#         verbose=False, \n",
    "#         gene_level_prior={'mean': 1, 'sd': 1/2, 'mean_var_ratio': 1},\n",
    "#         cell_number_prior={'cells_per_spot': n_nuclei.reshape(112,1), 'factors_per_spot': 5,\n",
    "#                            'cells_mean_var_ratio': 1, 'factors_mean_var_ratio': 1})\n",
    "\n",
    "# mod2 = LocationModelV7_V4_V4_GeoMx_V2(\n",
    "#         np.array(cellstateMatrix), np.array(counts_subset).T,\n",
    "#         data_type='float32', n_iter=2000,\n",
    "#         learning_rate=0.001,\n",
    "#         total_grad_norm_constraint=200,\n",
    "#         verbose=False, \n",
    "#         gene_level_prior={'mean': 1, 'sd': 1/2, 'mean_var_ratio': 1},\n",
    "#         cell_number_prior={'cells_per_spot': n_nuclei.reshape(112,1), 'factors_per_spot': 5,\n",
    "#                            'cells_mean_var_ratio': 1, 'factors_mean_var_ratio': 1})\n",
    "\n",
    "mod3 = LocationModelV7_V4_V4_GeoMx_V3(\n",
    "        np.array(cellstateMatrix), np.array(counts_subset).T, Y_data,\n",
    "        data_type='float32', n_iter=30000,\n",
    "        learning_rate=0.001,\n",
    "        total_grad_norm_constraint=200,\n",
    "        verbose=False, \n",
    "        gene_level_prior={'mean': 1, 'sd': 1/2, 'mean_var_ratio': 1},\n",
    "        cell_number_prior={'cells_per_spot': n_nuclei.reshape(112,1), 'factors_per_spot': 5,\n",
    "                           'cells_mean_var_ratio': 1, 'factors_mean_var_ratio': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod1.sample_prior()\n",
    "# mod1.plot_prior_vs_data()\n",
    "# plt.show()\n",
    "# mod2.sample_prior()\n",
    "# mod2.plot_prior_vs_data()\n",
    "# plt.show()\n",
    "# mod3.sample_prior()\n",
    "# mod3.plot_prior_vs_data()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30000 [00:00<?, ?it/s]\u001b[A\n",
      "Average Loss = 7.1122e+08:   0%|          | 0/30000 [00:00<?, ?it/s]\u001b[A\n",
      "Average Loss = 7.1122e+08:   0%|          | 0/30000 [00:00<?, ?it/s]\u001b[A\n",
      "Average Loss = 7.1122e+08:   0%|          | 4/30000 [00:00<12:57, 38.59it/s]\u001b[A\n",
      "Average Loss = 7.1122e+08:   0%|          | 7/30000 [00:00<16:01, 31.19it/s]\u001b[A\n",
      "Average Loss = 6.7496e+08:   0%|          | 7/30000 [00:00<16:01, 31.19it/s]\u001b[A\n",
      "Average Loss = 6.7496e+08:   0%|          | 7/30000 [00:00<16:01, 31.19it/s]\u001b[A\n",
      "Average Loss = 6.7496e+08:   0%|          | 12/30000 [00:00<15:48, 31.60it/s]\u001b[A\n",
      "Average Loss = 6.7496e+08:   0%|          | 15/30000 [00:00<17:28, 28.60it/s]\u001b[A\n",
      "Average Loss = 6.7496e+08:   0%|          | 19/30000 [00:00<17:05, 29.23it/s]\u001b[A\n",
      "Average Loss = 6.6635e+08:   0%|          | 19/30000 [00:00<17:05, 29.23it/s]\u001b[A\n",
      "Average Loss = 6.6635e+08:   0%|          | 19/30000 [00:00<17:05, 29.23it/s]\u001b[A\n",
      "Average Loss = 6.6635e+08:   0%|          | 22/30000 [00:00<18:20, 27.23it/s]\u001b[A\n",
      "Average Loss = 6.6635e+08:   0%|          | 28/30000 [00:00<15:56, 31.32it/s]\u001b[A\n",
      "Average Loss = 6.6297e+08:   0%|          | 28/30000 [00:01<15:56, 31.32it/s]\u001b[A\n",
      "Average Loss = 6.6297e+08:   0%|          | 28/30000 [00:01<15:56, 31.32it/s]\u001b[A\n",
      "Average Loss = 6.6297e+08:   0%|          | 32/30000 [00:01<16:20, 30.56it/s]\u001b[A\n",
      "Average Loss = 6.6297e+08:   0%|          | 38/30000 [00:01<14:40, 34.05it/s]\u001b[A\n",
      "Average Loss = 6.5709e+08:   0%|          | 38/30000 [00:01<14:40, 34.05it/s]\u001b[A\n",
      "Average Loss = 6.5709e+08:   0%|          | 38/30000 [00:01<14:40, 34.05it/s]\u001b[A\n",
      "Average Loss = 6.5709e+08:   0%|          | 42/30000 [00:01<15:55, 31.36it/s]\u001b[A\n",
      "Average Loss = 6.5709e+08:   0%|          | 47/30000 [00:01<14:39, 34.07it/s]\u001b[A\n",
      "Average Loss = 6.5112e+08:   0%|          | 47/30000 [00:01<14:39, 34.07it/s]\u001b[A\n",
      "Average Loss = 6.5112e+08:   0%|          | 47/30000 [00:01<14:39, 34.07it/s]\u001b[A\n",
      "Average Loss = 6.5112e+08:   0%|          | 51/30000 [00:01<15:28, 32.27it/s]\u001b[A\n",
      "Average Loss = 6.5112e+08:   0%|          | 57/30000 [00:01<14:11, 35.18it/s]\u001b[A\n",
      "Average Loss = 6.4259e+08:   0%|          | 57/30000 [00:01<14:11, 35.18it/s]\u001b[A\n",
      "Average Loss = 6.4259e+08:   0%|          | 57/30000 [00:01<14:11, 35.18it/s]\u001b[A\n",
      "Average Loss = 6.4259e+08:   0%|          | 61/30000 [00:01<15:48, 31.57it/s]\u001b[A\n",
      "Average Loss = 6.4259e+08:   0%|          | 65/30000 [00:01<15:11, 32.86it/s]\u001b[A\n",
      "Average Loss = 6.4259e+08:   0%|          | 69/30000 [00:02<15:31, 32.15it/s]\u001b[A\n",
      "Average Loss = 6.3936e+08:   0%|          | 69/30000 [00:02<15:31, 32.15it/s]\u001b[A\n",
      "Average Loss = 6.3936e+08:   0%|          | 69/30000 [00:02<15:31, 32.15it/s]\u001b[A\n",
      "Average Loss = 6.3936e+08:   0%|          | 74/30000 [00:02<14:34, 34.23it/s]\u001b[A\n",
      "Average Loss = 6.3936e+08:   0%|          | 78/30000 [00:02<15:16, 32.64it/s]\u001b[A\n",
      "Average Loss = 6.3397e+08:   0%|          | 78/30000 [00:02<15:16, 32.64it/s]\u001b[A\n",
      "Average Loss = 6.3397e+08:   0%|          | 78/30000 [00:02<15:16, 32.64it/s]\u001b[A\n",
      "Average Loss = 6.3397e+08:   0%|          | 82/30000 [00:02<15:05, 33.03it/s]\u001b[A\n",
      "Average Loss = 6.3397e+08:   0%|          | 86/30000 [00:02<15:39, 31.84it/s]\u001b[A\n",
      "Average Loss = 6.2984e+08:   0%|          | 86/30000 [00:02<15:39, 31.84it/s]\u001b[A\n",
      "Average Loss = 6.2984e+08:   0%|          | 86/30000 [00:02<15:39, 31.84it/s]\u001b[A\n",
      "Average Loss = 6.2984e+08:   0%|          | 92/30000 [00:02<14:09, 35.23it/s]\u001b[A\n",
      "Average Loss = 6.2984e+08:   0%|          | 96/30000 [00:02<14:57, 33.31it/s]\u001b[A\n",
      "Average Loss = 6.2551e+08:   0%|          | 96/30000 [00:03<14:57, 33.31it/s]\u001b[A\n",
      "Average Loss = 6.2551e+08:   0%|          | 96/30000 [00:03<14:57, 33.31it/s]\u001b[A\n",
      "Average Loss = 6.2551e+08:   0%|          | 101/30000 [00:03<14:09, 35.19it/s]\u001b[A\n",
      "Average Loss = 6.2551e+08:   0%|          | 105/30000 [00:03<14:59, 33.24it/s]\u001b[A\n",
      "Average Loss = 6.2194e+08:   0%|          | 105/30000 [00:03<14:59, 33.24it/s]\u001b[A\n",
      "Average Loss = 6.2194e+08:   0%|          | 105/30000 [00:03<14:59, 33.24it/s]\u001b[A\n",
      "Average Loss = 6.2194e+08:   0%|          | 111/30000 [00:03<13:48, 36.07it/s]\u001b[A\n",
      "Average Loss = 6.2194e+08:   0%|          | 119/30000 [00:03<11:37, 42.86it/s]\u001b[A\n",
      "Average Loss = 6.1862e+08:   0%|          | 119/30000 [00:03<11:37, 42.86it/s]\u001b[A\n",
      "Average Loss = 6.1862e+08:   0%|          | 119/30000 [00:03<11:37, 42.86it/s]\u001b[A\n",
      "Average Loss = 6.1862e+08:   0%|          | 124/30000 [00:03<12:47, 38.91it/s]\u001b[A\n",
      "Average Loss = 6.1862e+08:   0%|          | 129/30000 [00:03<13:29, 36.91it/s]\u001b[A\n",
      "Average Loss = 6.1462e+08:   0%|          | 129/30000 [00:03<13:29, 36.91it/s]\u001b[A\n",
      "Average Loss = 6.1462e+08:   0%|          | 129/30000 [00:03<13:29, 36.91it/s]\u001b[A\n",
      "Average Loss = 6.1462e+08:   0%|          | 134/30000 [00:03<13:14, 37.57it/s]\u001b[A\n",
      "Average Loss = 6.1462e+08:   0%|          | 139/30000 [00:03<12:45, 39.01it/s]\u001b[A\n",
      "Average Loss = 6.1076e+08:   0%|          | 139/30000 [00:04<12:45, 39.01it/s]\u001b[A\n",
      "Average Loss = 6.1076e+08:   0%|          | 139/30000 [00:04<12:45, 39.01it/s]\u001b[A\n",
      "Average Loss = 6.1076e+08:   0%|          | 144/30000 [00:04<13:30, 36.83it/s]\u001b[A\n",
      "Average Loss = 6.1076e+08:   0%|          | 148/30000 [00:04<13:26, 37.02it/s]\u001b[A\n",
      "Average Loss = 6.0676e+08:   0%|          | 148/30000 [00:04<13:26, 37.02it/s]\u001b[A\n",
      "Average Loss = 6.0676e+08:   0%|          | 148/30000 [00:04<13:26, 37.02it/s]\u001b[A\n",
      "Average Loss = 6.0676e+08:   1%|          | 152/30000 [00:04<14:54, 33.39it/s]\u001b[A\n",
      "Average Loss = 6.0676e+08:   1%|          | 156/30000 [00:04<14:20, 34.68it/s]\u001b[A\n",
      "Average Loss = 6.0676e+08:   1%|          | 160/30000 [00:04<14:53, 33.38it/s]\u001b[A\n",
      "Average Loss = 6.0279e+08:   1%|          | 160/30000 [00:04<14:53, 33.38it/s]\u001b[A\n",
      "Average Loss = 6.0279e+08:   1%|          | 160/30000 [00:04<14:53, 33.38it/s]\u001b[A\n",
      "Average Loss = 6.0279e+08:   1%|          | 165/30000 [00:04<14:09, 35.14it/s]\u001b[A\n",
      "Average Loss = 6.0279e+08:   1%|          | 169/30000 [00:04<15:03, 33.01it/s]\u001b[A\n",
      "Average Loss = 5.9912e+08:   1%|          | 169/30000 [00:04<15:03, 33.01it/s]\u001b[A\n",
      "Average Loss = 5.9912e+08:   1%|          | 169/30000 [00:04<15:03, 33.01it/s]\u001b[A\n",
      "Average Loss = 5.9912e+08:   1%|          | 173/30000 [00:04<15:10, 32.76it/s]\u001b[A\n",
      "Average Loss = 5.9912e+08:   1%|          | 177/30000 [00:05<15:38, 31.77it/s]\u001b[A\n",
      "Average Loss = 5.9516e+08:   1%|          | 177/30000 [00:05<15:38, 31.77it/s]\u001b[A\n",
      "Average Loss = 5.9516e+08:   1%|          | 177/30000 [00:05<15:38, 31.77it/s]\u001b[A\n",
      "Average Loss = 5.9516e+08:   1%|          | 182/30000 [00:05<14:24, 34.50it/s]\u001b[A\n",
      "Average Loss = 5.9516e+08:   1%|          | 186/30000 [00:05<14:57, 33.21it/s]\u001b[A\n",
      "Average Loss = 5.9148e+08:   1%|          | 186/30000 [00:05<14:57, 33.21it/s]\u001b[A\n",
      "Average Loss = 5.9148e+08:   1%|          | 186/30000 [00:05<14:57, 33.21it/s]\u001b[A\n",
      "Average Loss = 5.9148e+08:   1%|          | 192/30000 [00:05<13:48, 35.99it/s]\u001b[A\n",
      "Average Loss = 5.9148e+08:   1%|          | 196/30000 [00:05<14:49, 33.50it/s]\u001b[A\n",
      "Average Loss = 5.876e+08:   1%|          | 196/30000 [00:05<14:49, 33.50it/s] \u001b[A\n",
      "Average Loss = 5.876e+08:   1%|          | 196/30000 [00:05<14:49, 33.50it/s]\u001b[A\n",
      "Average Loss = 5.876e+08:   1%|          | 202/30000 [00:05<13:42, 36.25it/s]\u001b[A\n",
      "Average Loss = 5.876e+08:   1%|          | 206/30000 [00:05<14:37, 33.94it/s]\u001b[A\n",
      "Average Loss = 5.8362e+08:   1%|          | 206/30000 [00:06<14:37, 33.94it/s]\u001b[A\n",
      "Average Loss = 5.8362e+08:   1%|          | 206/30000 [00:06<14:37, 33.94it/s]\u001b[A\n",
      "Average Loss = 5.8362e+08:   1%|          | 211/30000 [00:06<13:56, 35.60it/s]\u001b[A\n",
      "Average Loss = 5.8362e+08:   1%|          | 215/30000 [00:06<15:22, 32.28it/s]\u001b[A\n",
      "Average Loss = 5.8362e+08:   1%|          | 219/30000 [00:06<14:39, 33.88it/s]\u001b[A\n",
      "Average Loss = 5.7984e+08:   1%|          | 219/30000 [00:06<14:39, 33.88it/s]\u001b[A\n",
      "Average Loss = 5.7984e+08:   1%|          | 219/30000 [00:06<14:39, 33.88it/s]\u001b[A\n",
      "Average Loss = 5.7984e+08:   1%|          | 223/30000 [00:06<15:05, 32.89it/s]\u001b[A\n",
      "Average Loss = 5.7984e+08:   1%|          | 229/30000 [00:06<13:44, 36.11it/s]\u001b[A\n",
      "Average Loss = 5.7668e+08:   1%|          | 229/30000 [00:06<13:44, 36.11it/s]\u001b[A\n",
      "Average Loss = 5.7668e+08:   1%|          | 229/30000 [00:06<13:44, 36.11it/s]\u001b[A\n",
      "Average Loss = 5.7668e+08:   1%|          | 233/30000 [00:06<14:27, 34.30it/s]\u001b[A\n",
      "Average Loss = 5.7668e+08:   1%|          | 238/30000 [00:06<14:14, 34.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 5.7323e+08:   1%|          | 238/30000 [00:06<14:14, 34.83it/s]\u001b[A\n",
      "Average Loss = 5.7323e+08:   1%|          | 238/30000 [00:06<14:14, 34.83it/s]\u001b[A\n",
      "Average Loss = 5.7323e+08:   1%|          | 242/30000 [00:06<15:21, 32.31it/s]\u001b[A\n",
      "Average Loss = 5.7323e+08:   1%|          | 247/30000 [00:07<14:13, 34.88it/s]\u001b[A\n",
      "Average Loss = 5.7044e+08:   1%|          | 247/30000 [00:07<14:13, 34.88it/s]\u001b[A\n",
      "Average Loss = 5.7044e+08:   1%|          | 247/30000 [00:07<14:13, 34.88it/s]\u001b[A\n",
      "Average Loss = 5.7044e+08:   1%|          | 251/30000 [00:07<15:13, 32.56it/s]\u001b[A\n",
      "Average Loss = 5.7044e+08:   1%|          | 256/30000 [00:07<14:22, 34.47it/s]\u001b[A\n",
      "Average Loss = 5.7044e+08:   1%|          | 260/30000 [00:07<15:16, 32.45it/s]\u001b[A\n",
      "Average Loss = 5.6723e+08:   1%|          | 260/30000 [00:07<15:16, 32.45it/s]\u001b[A\n",
      "Average Loss = 5.6723e+08:   1%|          | 260/30000 [00:07<15:16, 32.45it/s]\u001b[A\n",
      "Average Loss = 5.6723e+08:   1%|          | 264/30000 [00:07<14:56, 33.16it/s]\u001b[A\n",
      "Average Loss = 5.6723e+08:   1%|          | 268/30000 [00:07<15:32, 31.90it/s]\u001b[A\n",
      "Average Loss = 5.6416e+08:   1%|          | 268/30000 [00:07<15:32, 31.90it/s]\u001b[A\n",
      "Average Loss = 5.6416e+08:   1%|          | 268/30000 [00:07<15:32, 31.90it/s]\u001b[A\n",
      "Average Loss = 5.6416e+08:   1%|          | 273/30000 [00:07<14:36, 33.92it/s]\u001b[A\n",
      "Average Loss = 5.6416e+08:   1%|          | 277/30000 [00:07<15:48, 31.33it/s]\u001b[A\n",
      "Average Loss = 5.6115e+08:   1%|          | 277/30000 [00:08<15:48, 31.33it/s]\u001b[A\n",
      "Average Loss = 5.6115e+08:   1%|          | 277/30000 [00:08<15:48, 31.33it/s]\u001b[A\n",
      "Average Loss = 5.6115e+08:   1%|          | 281/30000 [00:08<15:27, 32.03it/s]\u001b[A\n",
      "Average Loss = 5.6115e+08:   1%|          | 285/30000 [00:08<15:36, 31.72it/s]\u001b[A\n",
      "Average Loss = 5.6115e+08:   1%|          | 290/30000 [00:08<14:24, 34.36it/s]\u001b[A\n",
      "Average Loss = 5.5788e+08:   1%|          | 290/30000 [00:08<14:24, 34.36it/s]\u001b[A\n",
      "Average Loss = 5.5788e+08:   1%|          | 290/30000 [00:08<14:24, 34.36it/s]\u001b[A\n",
      "Average Loss = 5.5788e+08:   1%|          | 294/30000 [00:08<15:19, 32.31it/s]\u001b[A\n",
      "Average Loss = 5.5788e+08:   1%|          | 299/30000 [00:08<14:36, 33.89it/s]\u001b[A\n",
      "Average Loss = 5.5507e+08:   1%|          | 299/30000 [00:08<14:36, 33.89it/s]\u001b[A\n",
      "Average Loss = 5.5507e+08:   1%|          | 299/30000 [00:08<14:36, 33.89it/s]\u001b[A\n",
      "Average Loss = 5.5507e+08:   1%|          | 303/30000 [00:08<15:12, 32.53it/s]\u001b[A\n",
      "Average Loss = 5.5507e+08:   1%|          | 308/30000 [00:08<14:17, 34.64it/s]\u001b[A\n",
      "Average Loss = 5.5242e+08:   1%|          | 308/30000 [00:09<14:17, 34.64it/s]\u001b[A\n",
      "Average Loss = 5.5242e+08:   1%|          | 308/30000 [00:09<14:17, 34.64it/s]\u001b[A\n",
      "Average Loss = 5.5242e+08:   1%|          | 312/30000 [00:09<15:21, 32.21it/s]\u001b[A\n",
      "Average Loss = 5.5242e+08:   1%|          | 317/30000 [00:09<14:16, 34.68it/s]\u001b[A\n",
      "Average Loss = 5.4966e+08:   1%|          | 317/30000 [00:09<14:16, 34.68it/s]\u001b[A\n",
      "Average Loss = 5.4966e+08:   1%|          | 317/30000 [00:09<14:16, 34.68it/s]\u001b[A\n",
      "Average Loss = 5.4966e+08:   1%|          | 321/30000 [00:09<15:12, 32.51it/s]\u001b[A\n",
      "Average Loss = 5.4966e+08:   1%|          | 326/30000 [00:09<14:09, 34.92it/s]\u001b[A\n",
      "Average Loss = 5.4966e+08:   1%|          | 330/30000 [00:09<14:57, 33.05it/s]\u001b[A\n",
      "Average Loss = 5.4672e+08:   1%|          | 330/30000 [00:09<14:57, 33.05it/s]\u001b[A\n",
      "Average Loss = 5.4672e+08:   1%|          | 330/30000 [00:09<14:57, 33.05it/s]\u001b[A\n",
      "Average Loss = 5.4672e+08:   1%|          | 335/30000 [00:09<14:11, 34.86it/s]\u001b[A\n",
      "Average Loss = 5.4672e+08:   1%|          | 339/30000 [00:09<15:28, 31.95it/s]\u001b[A\n",
      "Average Loss = 5.438e+08:   1%|          | 339/30000 [00:09<15:28, 31.95it/s] \u001b[A\n",
      "Average Loss = 5.438e+08:   1%|          | 339/30000 [00:09<15:28, 31.95it/s]\u001b[A\n",
      "Average Loss = 5.438e+08:   1%|          | 343/30000 [00:09<15:07, 32.69it/s]\u001b[A\n",
      "Average Loss = 5.438e+08:   1%|          | 347/30000 [00:10<15:24, 32.09it/s]\u001b[A\n",
      "Average Loss = 5.411e+08:   1%|          | 347/30000 [00:10<15:24, 32.09it/s]\u001b[A\n",
      "Average Loss = 5.411e+08:   1%|          | 347/30000 [00:10<15:24, 32.09it/s]\u001b[A\n",
      "Average Loss = 5.411e+08:   1%|          | 352/30000 [00:10<14:14, 34.70it/s]\u001b[A\n",
      "Average Loss = 5.411e+08:   1%|          | 356/30000 [00:10<14:49, 33.31it/s]\u001b[A\n",
      "Average Loss = 5.3855e+08:   1%|          | 356/30000 [00:10<14:49, 33.31it/s]\u001b[A\n",
      "Average Loss = 5.3855e+08:   1%|          | 356/30000 [00:10<14:49, 33.31it/s]\u001b[A\n",
      "Average Loss = 5.3855e+08:   1%|          | 361/30000 [00:10<13:27, 36.70it/s]\u001b[A\n",
      "Average Loss = 5.3855e+08:   1%|          | 365/30000 [00:10<15:10, 32.55it/s]\u001b[A\n",
      "Average Loss = 5.3577e+08:   1%|          | 365/30000 [00:10<15:10, 32.55it/s]\u001b[A\n",
      "Average Loss = 5.3577e+08:   1%|          | 365/30000 [00:10<15:10, 32.55it/s]\u001b[A\n",
      "Average Loss = 5.3577e+08:   1%|          | 371/30000 [00:10<13:56, 35.42it/s]\u001b[A\n",
      "Average Loss = 5.3577e+08:   1%|▏         | 375/30000 [00:10<14:51, 33.22it/s]\u001b[A\n",
      "Average Loss = 5.3577e+08:   1%|▏         | 380/30000 [00:10<13:54, 35.48it/s]\u001b[A\n",
      "Average Loss = 5.3331e+08:   1%|▏         | 380/30000 [00:10<13:54, 35.48it/s]\u001b[A\n",
      "Average Loss = 5.3331e+08:   1%|▏         | 380/30000 [00:10<13:54, 35.48it/s]\u001b[A\n",
      "Average Loss = 5.3331e+08:   1%|▏         | 384/30000 [00:11<15:00, 32.88it/s]\u001b[A\n",
      "Average Loss = 5.3331e+08:   1%|▏         | 389/30000 [00:11<13:59, 35.26it/s]\u001b[A\n",
      "Average Loss = 5.3061e+08:   1%|▏         | 389/30000 [00:11<13:59, 35.26it/s]\u001b[A\n",
      "Average Loss = 5.3061e+08:   1%|▏         | 389/30000 [00:11<13:59, 35.26it/s]\u001b[A\n",
      "Average Loss = 5.3061e+08:   1%|▏         | 393/30000 [00:11<14:38, 33.70it/s]\u001b[A\n",
      "Average Loss = 5.3061e+08:   1%|▏         | 399/30000 [00:11<13:26, 36.70it/s]\u001b[A\n",
      "Average Loss = 5.278e+08:   1%|▏         | 399/30000 [00:11<13:26, 36.70it/s] \u001b[A\n",
      "Average Loss = 5.278e+08:   1%|▏         | 399/30000 [00:11<13:26, 36.70it/s]\u001b[A\n",
      "Average Loss = 5.278e+08:   1%|▏         | 403/30000 [00:11<14:24, 34.26it/s]\u001b[A\n",
      "Average Loss = 5.278e+08:   1%|▏         | 408/30000 [00:11<14:00, 35.23it/s]\u001b[A\n",
      "Average Loss = 5.2525e+08:   1%|▏         | 408/30000 [00:11<14:00, 35.23it/s]\u001b[A\n",
      "Average Loss = 5.2525e+08:   1%|▏         | 408/30000 [00:11<14:00, 35.23it/s]\u001b[A\n",
      "Average Loss = 5.2525e+08:   1%|▏         | 412/30000 [00:11<15:07, 32.60it/s]\u001b[A\n",
      "Average Loss = 5.2525e+08:   1%|▏         | 417/30000 [00:12<14:04, 35.03it/s]\u001b[A\n",
      "Average Loss = 5.2269e+08:   1%|▏         | 417/30000 [00:12<14:04, 35.03it/s]\u001b[A\n",
      "Average Loss = 5.2269e+08:   1%|▏         | 417/30000 [00:12<14:04, 35.03it/s]\u001b[A\n",
      "Average Loss = 5.2269e+08:   1%|▏         | 421/30000 [00:12<15:06, 32.62it/s]\u001b[A\n",
      "Average Loss = 5.2269e+08:   1%|▏         | 426/30000 [00:12<14:14, 34.61it/s]\u001b[A\n",
      "Average Loss = 5.2269e+08:   1%|▏         | 430/30000 [00:12<14:47, 33.32it/s]\u001b[A\n",
      "Average Loss = 5.2023e+08:   1%|▏         | 430/30000 [00:12<14:47, 33.32it/s]\u001b[A\n",
      "Average Loss = 5.2023e+08:   1%|▏         | 430/30000 [00:12<14:47, 33.32it/s]\u001b[A\n",
      "Average Loss = 5.2023e+08:   1%|▏         | 435/30000 [00:12<14:05, 34.99it/s]\u001b[A\n",
      "Average Loss = 5.2023e+08:   1%|▏         | 439/30000 [00:12<14:53, 33.07it/s]\u001b[A\n",
      "Average Loss = 5.1777e+08:   1%|▏         | 439/30000 [00:12<14:53, 33.07it/s]\u001b[A\n",
      "Average Loss = 5.1777e+08:   1%|▏         | 439/30000 [00:12<14:53, 33.07it/s]\u001b[A\n",
      "Average Loss = 5.1777e+08:   1%|▏         | 444/30000 [00:12<14:27, 34.08it/s]\u001b[A\n",
      "Average Loss = 5.1777e+08:   1%|▏         | 448/30000 [00:12<15:28, 31.82it/s]\u001b[A\n",
      "Average Loss = 5.1534e+08:   1%|▏         | 448/30000 [00:13<15:28, 31.82it/s]\u001b[A\n",
      "Average Loss = 5.1534e+08:   1%|▏         | 448/30000 [00:13<15:28, 31.82it/s]\u001b[A\n",
      "Average Loss = 5.1534e+08:   2%|▏         | 452/30000 [00:13<15:00, 32.81it/s]\u001b[A\n",
      "Average Loss = 5.1534e+08:   2%|▏         | 456/30000 [00:13<15:14, 32.32it/s]\u001b[A\n",
      "Average Loss = 5.1291e+08:   2%|▏         | 456/30000 [00:13<15:14, 32.32it/s]\u001b[A\n",
      "Average Loss = 5.1291e+08:   2%|▏         | 456/30000 [00:13<15:14, 32.32it/s]\u001b[A\n",
      "Average Loss = 5.1291e+08:   2%|▏         | 462/30000 [00:13<13:48, 35.67it/s]\u001b[A\n",
      "Average Loss = 5.1291e+08:   2%|▏         | 466/30000 [00:13<14:39, 33.58it/s]\u001b[A\n",
      "Average Loss = 5.1291e+08:   2%|▏         | 470/30000 [00:13<14:35, 33.74it/s]\u001b[A\n",
      "Average Loss = 5.1055e+08:   2%|▏         | 470/30000 [00:13<14:35, 33.74it/s]\u001b[A\n",
      "Average Loss = 5.1055e+08:   2%|▏         | 470/30000 [00:13<14:35, 33.74it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 5.1055e+08:   2%|▏         | 474/30000 [00:13<15:17, 32.17it/s]\u001b[A\n",
      "Average Loss = 5.1055e+08:   2%|▏         | 480/30000 [00:13<13:51, 35.50it/s]\u001b[A\n",
      "Average Loss = 5.0828e+08:   2%|▏         | 480/30000 [00:13<13:51, 35.50it/s]\u001b[A\n",
      "Average Loss = 5.0828e+08:   2%|▏         | 480/30000 [00:13<13:51, 35.50it/s]\u001b[A\n",
      "Average Loss = 5.0828e+08:   2%|▏         | 484/30000 [00:13<14:53, 33.02it/s]\u001b[A\n",
      "Average Loss = 5.0828e+08:   2%|▏         | 489/30000 [00:14<14:17, 34.43it/s]\u001b[A\n",
      "Average Loss = 5.0602e+08:   2%|▏         | 489/30000 [00:14<14:17, 34.43it/s]\u001b[A\n",
      "Average Loss = 5.0602e+08:   2%|▏         | 489/30000 [00:14<14:17, 34.43it/s]\u001b[A\n",
      "Average Loss = 5.0602e+08:   2%|▏         | 493/30000 [00:14<15:11, 32.35it/s]\u001b[A\n",
      "Average Loss = 5.0602e+08:   2%|▏         | 497/30000 [00:14<14:32, 33.83it/s]\u001b[A\n",
      "Average Loss = 5.0383e+08:   2%|▏         | 497/30000 [00:14<14:32, 33.83it/s]\u001b[A\n",
      "Average Loss = 5.0383e+08:   2%|▏         | 497/30000 [00:14<14:32, 33.83it/s]\u001b[A\n",
      "Average Loss = 5.0383e+08:   2%|▏         | 501/30000 [00:14<15:10, 32.38it/s]\u001b[A\n",
      "Average Loss = 5.0383e+08:   2%|▏         | 507/30000 [00:14<13:51, 35.48it/s]\u001b[A\n",
      "Average Loss = 5.0155e+08:   2%|▏         | 507/30000 [00:14<13:51, 35.48it/s]\u001b[A\n",
      "Average Loss = 5.0155e+08:   2%|▏         | 507/30000 [00:14<13:51, 35.48it/s]\u001b[A\n",
      "Average Loss = 5.0155e+08:   2%|▏         | 511/30000 [00:14<15:21, 32.00it/s]\u001b[A\n",
      "Average Loss = 5.0155e+08:   2%|▏         | 515/30000 [00:14<14:36, 33.63it/s]\u001b[A\n",
      "Average Loss = 5.0155e+08:   2%|▏         | 519/30000 [00:15<15:10, 32.37it/s]\u001b[A\n",
      "Average Loss = 4.9952e+08:   2%|▏         | 519/30000 [00:15<15:10, 32.37it/s]\u001b[A\n",
      "Average Loss = 4.9952e+08:   2%|▏         | 519/30000 [00:15<15:10, 32.37it/s]\u001b[A\n",
      "Average Loss = 4.9952e+08:   2%|▏         | 525/30000 [00:15<13:59, 35.09it/s]\u001b[A\n",
      "Average Loss = 4.9952e+08:   2%|▏         | 529/30000 [00:15<15:29, 31.71it/s]\u001b[A\n",
      "Average Loss = 4.9734e+08:   2%|▏         | 529/30000 [00:15<15:29, 31.71it/s]\u001b[A\n",
      "Average Loss = 4.9734e+08:   2%|▏         | 529/30000 [00:15<15:29, 31.71it/s]\u001b[A\n",
      "Average Loss = 4.9734e+08:   2%|▏         | 533/30000 [00:15<15:20, 32.00it/s]\u001b[A\n",
      "Average Loss = 4.9734e+08:   2%|▏         | 537/30000 [00:15<15:39, 31.36it/s]\u001b[A\n",
      "Average Loss = 4.9524e+08:   2%|▏         | 537/30000 [00:15<15:39, 31.36it/s]\u001b[A\n",
      "Average Loss = 4.9524e+08:   2%|▏         | 537/30000 [00:15<15:39, 31.36it/s]\u001b[A\n",
      "Average Loss = 4.9524e+08:   2%|▏         | 542/30000 [00:15<14:25, 34.04it/s]\u001b[A\n",
      "Average Loss = 4.9524e+08:   2%|▏         | 546/30000 [00:15<15:07, 32.47it/s]\u001b[A\n",
      "Average Loss = 4.9315e+08:   2%|▏         | 546/30000 [00:15<15:07, 32.47it/s]\u001b[A\n",
      "Average Loss = 4.9315e+08:   2%|▏         | 546/30000 [00:15<15:07, 32.47it/s]\u001b[A\n",
      "Average Loss = 4.9315e+08:   2%|▏         | 551/30000 [00:15<14:18, 34.31it/s]\u001b[A\n",
      "Average Loss = 4.9315e+08:   2%|▏         | 555/30000 [00:16<15:03, 32.57it/s]\u001b[A\n",
      "Average Loss = 4.9097e+08:   2%|▏         | 555/30000 [00:16<15:03, 32.57it/s]\u001b[A\n",
      "Average Loss = 4.9097e+08:   2%|▏         | 555/30000 [00:16<15:03, 32.57it/s]\u001b[A\n",
      "Average Loss = 4.9097e+08:   2%|▏         | 561/30000 [00:16<13:58, 35.12it/s]\u001b[A\n",
      "Average Loss = 4.9097e+08:   2%|▏         | 565/30000 [00:16<14:31, 33.76it/s]\u001b[A\n",
      "Average Loss = 4.9097e+08:   2%|▏         | 570/30000 [00:16<13:36, 36.03it/s]\u001b[A\n",
      "Average Loss = 4.8897e+08:   2%|▏         | 570/30000 [00:16<13:36, 36.03it/s]\u001b[A\n",
      "Average Loss = 4.8897e+08:   2%|▏         | 570/30000 [00:16<13:36, 36.03it/s]\u001b[A\n",
      "Average Loss = 4.8897e+08:   2%|▏         | 574/30000 [00:16<14:46, 33.19it/s]\u001b[A\n",
      "Average Loss = 4.8691e+08:   2%|▏         | 574/30000 [00:16<14:46, 33.19it/s]\u001b[A\n",
      "Average Loss = 4.8691e+08:   2%|▏         | 574/30000 [00:16<14:46, 33.19it/s]\u001b[A\n",
      "Average Loss = 4.8691e+08:   2%|▏         | 581/30000 [00:16<13:23, 36.60it/s]\u001b[A\n",
      "Average Loss = 4.8691e+08:   2%|▏         | 585/30000 [00:16<14:18, 34.28it/s]\u001b[A\n",
      "Average Loss = 4.8691e+08:   2%|▏         | 590/30000 [00:17<13:21, 36.70it/s]\u001b[A\n",
      "Average Loss = 4.85e+08:   2%|▏         | 590/30000 [00:17<13:21, 36.70it/s]  \u001b[A\n",
      "Average Loss = 4.85e+08:   2%|▏         | 590/30000 [00:17<13:21, 36.70it/s]\u001b[A\n",
      "Average Loss = 4.85e+08:   2%|▏         | 594/30000 [00:17<14:37, 33.51it/s]\u001b[A\n",
      "Average Loss = 4.85e+08:   2%|▏         | 598/30000 [00:17<14:42, 33.32it/s]\u001b[A\n",
      "Average Loss = 4.8288e+08:   2%|▏         | 598/30000 [00:17<14:42, 33.32it/s]\u001b[A\n",
      "Average Loss = 4.8288e+08:   2%|▏         | 598/30000 [00:17<14:42, 33.32it/s]\u001b[A\n",
      "Average Loss = 4.8288e+08:   2%|▏         | 602/30000 [00:17<15:21, 31.91it/s]\u001b[A\n",
      "Average Loss = 4.8288e+08:   2%|▏         | 608/30000 [00:17<13:57, 35.11it/s]\u001b[A\n",
      "Average Loss = 4.8094e+08:   2%|▏         | 608/30000 [00:17<13:57, 35.11it/s]\u001b[A\n",
      "Average Loss = 4.8094e+08:   2%|▏         | 608/30000 [00:17<13:57, 35.11it/s]\u001b[A\n",
      "Average Loss = 4.8094e+08:   2%|▏         | 612/30000 [00:17<15:07, 32.37it/s]\u001b[A\n",
      "Average Loss = 4.8094e+08:   2%|▏         | 616/30000 [00:17<14:31, 33.71it/s]\u001b[A\n",
      "Average Loss = 4.8094e+08:   2%|▏         | 620/30000 [00:17<15:24, 31.77it/s]\u001b[A\n",
      "Average Loss = 4.7908e+08:   2%|▏         | 620/30000 [00:17<15:24, 31.77it/s]\u001b[A\n",
      "Average Loss = 4.7908e+08:   2%|▏         | 620/30000 [00:17<15:24, 31.77it/s]\u001b[A\n",
      "Average Loss = 4.7908e+08:   2%|▏         | 625/30000 [00:18<14:32, 33.66it/s]\u001b[A\n",
      "Average Loss = 4.7908e+08:   2%|▏         | 629/30000 [00:18<15:10, 32.24it/s]\u001b[A\n",
      "Average Loss = 4.7717e+08:   2%|▏         | 629/30000 [00:18<15:10, 32.24it/s]\u001b[A\n",
      "Average Loss = 4.7717e+08:   2%|▏         | 629/30000 [00:18<15:10, 32.24it/s]\u001b[A\n",
      "Average Loss = 4.7717e+08:   2%|▏         | 634/30000 [00:18<14:18, 34.22it/s]\u001b[A\n",
      "Average Loss = 4.7717e+08:   2%|▏         | 638/30000 [00:18<15:14, 32.11it/s]\u001b[A\n",
      "Average Loss = 4.7531e+08:   2%|▏         | 638/30000 [00:18<15:14, 32.11it/s]\u001b[A\n",
      "Average Loss = 4.7531e+08:   2%|▏         | 638/30000 [00:18<15:14, 32.11it/s]\u001b[A\n",
      "Average Loss = 4.7531e+08:   2%|▏         | 642/30000 [00:18<14:35, 33.53it/s]\u001b[A\n",
      "Average Loss = 4.7531e+08:   2%|▏         | 646/30000 [00:18<15:13, 32.13it/s]\u001b[A\n",
      "Average Loss = 4.7339e+08:   2%|▏         | 646/30000 [00:18<15:13, 32.13it/s]\u001b[A\n",
      "Average Loss = 4.7339e+08:   2%|▏         | 646/30000 [00:18<15:13, 32.13it/s]\u001b[A\n",
      "Average Loss = 4.7339e+08:   2%|▏         | 652/30000 [00:18<13:47, 35.45it/s]\u001b[A\n",
      "Average Loss = 4.7339e+08:   2%|▏         | 656/30000 [00:18<14:34, 33.56it/s]\u001b[A\n",
      "Average Loss = 4.715e+08:   2%|▏         | 656/30000 [00:19<14:34, 33.56it/s] \u001b[A\n",
      "Average Loss = 4.715e+08:   2%|▏         | 656/30000 [00:19<14:34, 33.56it/s]\u001b[A\n",
      "Average Loss = 4.715e+08:   2%|▏         | 661/30000 [00:19<14:17, 34.20it/s]\u001b[A\n",
      "Average Loss = 4.715e+08:   2%|▏         | 665/30000 [00:19<14:43, 33.19it/s]\u001b[A\n",
      "Average Loss = 4.715e+08:   2%|▏         | 670/30000 [00:19<13:45, 35.55it/s]\u001b[A\n",
      "Average Loss = 4.6969e+08:   2%|▏         | 670/30000 [00:19<13:45, 35.55it/s]\u001b[A\n",
      "Average Loss = 4.6969e+08:   2%|▏         | 670/30000 [00:19<13:45, 35.55it/s]\u001b[A\n",
      "Average Loss = 4.6969e+08:   2%|▏         | 674/30000 [00:19<14:46, 33.07it/s]\u001b[A\n",
      "Average Loss = 4.6969e+08:   2%|▏         | 679/30000 [00:19<13:48, 35.40it/s]\u001b[A\n",
      "Average Loss = 4.6789e+08:   2%|▏         | 679/30000 [00:19<13:48, 35.40it/s]\u001b[A\n",
      "Average Loss = 4.6789e+08:   2%|▏         | 679/30000 [00:19<13:48, 35.40it/s]\u001b[A\n",
      "Average Loss = 4.6789e+08:   2%|▏         | 683/30000 [00:19<15:09, 32.24it/s]\u001b[A\n",
      "Average Loss = 4.6789e+08:   2%|▏         | 688/30000 [00:19<14:06, 34.64it/s]\u001b[A\n",
      "Average Loss = 4.6613e+08:   2%|▏         | 688/30000 [00:20<14:06, 34.64it/s]\u001b[A\n",
      "Average Loss = 4.6613e+08:   2%|▏         | 688/30000 [00:20<14:06, 34.64it/s]\u001b[A\n",
      "Average Loss = 4.6613e+08:   2%|▏         | 692/30000 [00:20<15:08, 32.25it/s]\u001b[A\n",
      "Average Loss = 4.6613e+08:   2%|▏         | 697/30000 [00:20<14:02, 34.79it/s]\u001b[A\n",
      "Average Loss = 4.644e+08:   2%|▏         | 697/30000 [00:20<14:02, 34.79it/s] \u001b[A\n",
      "Average Loss = 4.644e+08:   2%|▏         | 697/30000 [00:20<14:02, 34.79it/s]\u001b[A\n",
      "Average Loss = 4.644e+08:   2%|▏         | 701/30000 [00:20<15:23, 31.73it/s]\u001b[A\n",
      "Average Loss = 4.644e+08:   2%|▏         | 705/30000 [00:20<14:49, 32.92it/s]\u001b[A\n",
      "Average Loss = 4.644e+08:   2%|▏         | 709/30000 [00:20<15:22, 31.75it/s]\u001b[A\n",
      "Average Loss = 4.6266e+08:   2%|▏         | 709/30000 [00:20<15:22, 31.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 4.6266e+08:   2%|▏         | 709/30000 [00:20<15:22, 31.75it/s]\u001b[A\n",
      "Average Loss = 4.6266e+08:   2%|▏         | 714/30000 [00:20<14:27, 33.75it/s]\u001b[A\n",
      "Average Loss = 4.6266e+08:   2%|▏         | 718/30000 [00:20<15:19, 31.83it/s]\u001b[A\n",
      "Average Loss = 4.6099e+08:   2%|▏         | 718/30000 [00:20<15:19, 31.83it/s]\u001b[A\n",
      "Average Loss = 4.6099e+08:   2%|▏         | 718/30000 [00:20<15:19, 31.83it/s]\u001b[A\n",
      "Average Loss = 4.6099e+08:   2%|▏         | 722/30000 [00:20<15:04, 32.36it/s]\u001b[A\n",
      "Average Loss = 4.6099e+08:   2%|▏         | 726/30000 [00:21<15:32, 31.38it/s]\u001b[A\n",
      "Average Loss = 4.5929e+08:   2%|▏         | 726/30000 [00:21<15:32, 31.38it/s]\u001b[A\n",
      "Average Loss = 4.5929e+08:   2%|▏         | 726/30000 [00:21<15:32, 31.38it/s]\u001b[A\n",
      "Average Loss = 4.5929e+08:   2%|▏         | 731/30000 [00:21<14:28, 33.69it/s]\u001b[A\n",
      "Average Loss = 4.5929e+08:   2%|▏         | 735/30000 [00:21<15:11, 32.09it/s]\u001b[A\n",
      "Average Loss = 4.5929e+08:   2%|▏         | 740/30000 [00:21<14:04, 34.64it/s]\u001b[A\n",
      "Average Loss = 4.576e+08:   2%|▏         | 740/30000 [00:21<14:04, 34.64it/s] \u001b[A\n",
      "Average Loss = 4.576e+08:   2%|▏         | 740/30000 [00:21<14:04, 34.64it/s]\u001b[A\n",
      "Average Loss = 4.576e+08:   2%|▏         | 744/30000 [00:21<15:02, 32.41it/s]\u001b[A\n",
      "Average Loss = 4.576e+08:   2%|▏         | 749/30000 [00:21<13:57, 34.92it/s]\u001b[A\n",
      "Average Loss = 4.559e+08:   2%|▏         | 749/30000 [00:21<13:57, 34.92it/s]\u001b[A\n",
      "Average Loss = 4.559e+08:   2%|▏         | 749/30000 [00:21<13:57, 34.92it/s]\u001b[A\n",
      "Average Loss = 4.559e+08:   3%|▎         | 753/30000 [00:21<14:31, 33.55it/s]\u001b[A\n",
      "Average Loss = 4.559e+08:   3%|▎         | 759/30000 [00:21<13:20, 36.55it/s]\u001b[A\n",
      "Average Loss = 4.5422e+08:   3%|▎         | 759/30000 [00:22<13:20, 36.55it/s]\u001b[A\n",
      "Average Loss = 4.5422e+08:   3%|▎         | 759/30000 [00:22<13:20, 36.55it/s]\u001b[A\n",
      "Average Loss = 4.5422e+08:   3%|▎         | 763/30000 [00:22<14:26, 33.75it/s]\u001b[A\n",
      "Average Loss = 4.5422e+08:   3%|▎         | 768/30000 [00:22<13:36, 35.79it/s]\u001b[A\n",
      "Average Loss = 4.5264e+08:   3%|▎         | 768/30000 [00:22<13:36, 35.79it/s]\u001b[A\n",
      "Average Loss = 4.5264e+08:   3%|▎         | 768/30000 [00:22<13:36, 35.79it/s]\u001b[A\n",
      "Average Loss = 4.5264e+08:   3%|▎         | 772/30000 [00:22<14:33, 33.47it/s]\u001b[A\n",
      "Average Loss = 4.5264e+08:   3%|▎         | 778/30000 [00:22<13:26, 36.24it/s]\u001b[A\n",
      "Average Loss = 4.51e+08:   3%|▎         | 778/30000 [00:22<13:26, 36.24it/s]  \u001b[A\n",
      "Average Loss = 4.51e+08:   3%|▎         | 778/30000 [00:22<13:26, 36.24it/s]\u001b[A\n",
      "Average Loss = 4.51e+08:   3%|▎         | 782/30000 [00:22<14:51, 32.77it/s]\u001b[A\n",
      "Average Loss = 4.51e+08:   3%|▎         | 786/30000 [00:22<14:39, 33.22it/s]\u001b[A\n",
      "Average Loss = 4.51e+08:   3%|▎         | 790/30000 [00:22<15:06, 32.21it/s]\u001b[A\n",
      "Average Loss = 4.4939e+08:   3%|▎         | 790/30000 [00:22<15:06, 32.21it/s]\u001b[A\n",
      "Average Loss = 4.4939e+08:   3%|▎         | 790/30000 [00:22<15:06, 32.21it/s]\u001b[A\n",
      "Average Loss = 4.4939e+08:   3%|▎         | 794/30000 [00:23<15:07, 32.20it/s]\u001b[A\n",
      "Average Loss = 4.4939e+08:   3%|▎         | 798/30000 [00:23<15:34, 31.26it/s]\u001b[A\n",
      "Average Loss = 4.4782e+08:   3%|▎         | 798/30000 [00:23<15:34, 31.26it/s]\u001b[A\n",
      "Average Loss = 4.4782e+08:   3%|▎         | 798/30000 [00:23<15:34, 31.26it/s]\u001b[A\n",
      "Average Loss = 4.4782e+08:   3%|▎         | 803/30000 [00:23<14:46, 32.95it/s]\u001b[A\n",
      "Average Loss = 4.4782e+08:   3%|▎         | 807/30000 [00:23<15:18, 31.77it/s]\u001b[A\n",
      "Average Loss = 4.463e+08:   3%|▎         | 807/30000 [00:23<15:18, 31.77it/s] \u001b[A\n",
      "Average Loss = 4.463e+08:   3%|▎         | 807/30000 [00:23<15:18, 31.77it/s]\u001b[A\n",
      "Average Loss = 4.463e+08:   3%|▎         | 811/30000 [00:23<14:46, 32.92it/s]\u001b[A\n",
      "Average Loss = 4.463e+08:   3%|▎         | 815/30000 [00:23<15:23, 31.61it/s]\u001b[A\n",
      "Average Loss = 4.463e+08:   3%|▎         | 820/30000 [00:23<14:12, 34.22it/s]\u001b[A\n",
      "Average Loss = 4.4475e+08:   3%|▎         | 820/30000 [00:23<14:12, 34.22it/s]\u001b[A\n",
      "Average Loss = 4.4475e+08:   3%|▎         | 820/30000 [00:23<14:12, 34.22it/s]\u001b[A\n",
      "Average Loss = 4.4475e+08:   3%|▎         | 824/30000 [00:23<15:18, 31.77it/s]\u001b[A\n",
      "Average Loss = 4.4475e+08:   3%|▎         | 828/30000 [00:24<14:49, 32.79it/s]\u001b[A\n",
      "Average Loss = 4.4311e+08:   3%|▎         | 828/30000 [00:24<14:49, 32.79it/s]\u001b[A\n",
      "Average Loss = 4.4311e+08:   3%|▎         | 828/30000 [00:24<14:49, 32.79it/s]\u001b[A\n",
      "Average Loss = 4.4311e+08:   3%|▎         | 832/30000 [00:24<15:21, 31.65it/s]\u001b[A\n",
      "Average Loss = 4.4311e+08:   3%|▎         | 838/30000 [00:24<13:54, 34.95it/s]\u001b[A\n",
      "Average Loss = 4.4155e+08:   3%|▎         | 838/30000 [00:24<13:54, 34.95it/s]\u001b[A\n",
      "Average Loss = 4.4155e+08:   3%|▎         | 838/30000 [00:24<13:54, 34.95it/s]\u001b[A\n",
      "Average Loss = 4.4155e+08:   3%|▎         | 842/30000 [00:24<15:04, 32.25it/s]\u001b[A\n",
      "Average Loss = 4.4155e+08:   3%|▎         | 846/30000 [00:24<14:45, 32.91it/s]\u001b[A\n",
      "Average Loss = 4.4155e+08:   3%|▎         | 850/30000 [00:24<15:18, 31.75it/s]\u001b[A\n",
      "Average Loss = 4.4002e+08:   3%|▎         | 850/30000 [00:24<15:18, 31.75it/s]\u001b[A\n",
      "Average Loss = 4.4002e+08:   3%|▎         | 850/30000 [00:24<15:18, 31.75it/s]\u001b[A\n",
      "Average Loss = 4.4002e+08:   3%|▎         | 855/30000 [00:24<14:21, 33.84it/s]\u001b[A\n",
      "Average Loss = 4.4002e+08:   3%|▎         | 859/30000 [00:24<14:58, 32.44it/s]\u001b[A\n",
      "Average Loss = 4.3854e+08:   3%|▎         | 859/30000 [00:25<14:58, 32.44it/s]\u001b[A\n",
      "Average Loss = 4.3854e+08:   3%|▎         | 859/30000 [00:25<14:58, 32.44it/s]\u001b[A\n",
      "Average Loss = 4.3854e+08:   3%|▎         | 864/30000 [00:25<14:29, 33.52it/s]\u001b[A\n",
      "Average Loss = 4.3854e+08:   3%|▎         | 868/30000 [00:25<15:13, 31.88it/s]\u001b[A\n",
      "Average Loss = 4.3707e+08:   3%|▎         | 868/30000 [00:25<15:13, 31.88it/s]\u001b[A\n",
      "Average Loss = 4.3707e+08:   3%|▎         | 868/30000 [00:25<15:13, 31.88it/s]\u001b[A\n",
      "Average Loss = 4.3707e+08:   3%|▎         | 872/30000 [00:25<14:29, 33.48it/s]\u001b[A\n",
      "Average Loss = 4.3707e+08:   3%|▎         | 876/30000 [00:25<14:55, 32.52it/s]\u001b[A\n",
      "Average Loss = 4.3554e+08:   3%|▎         | 876/30000 [00:25<14:55, 32.52it/s]\u001b[A\n",
      "Average Loss = 4.3554e+08:   3%|▎         | 876/30000 [00:25<14:55, 32.52it/s]\u001b[A\n",
      "Average Loss = 4.3554e+08:   3%|▎         | 882/30000 [00:25<13:36, 35.68it/s]\u001b[A\n",
      "Average Loss = 4.3554e+08:   3%|▎         | 886/30000 [00:25<14:34, 33.28it/s]\u001b[A\n",
      "Average Loss = 4.3408e+08:   3%|▎         | 886/30000 [00:25<14:34, 33.28it/s]\u001b[A\n",
      "Average Loss = 4.3408e+08:   3%|▎         | 886/30000 [00:25<14:34, 33.28it/s]\u001b[A\n",
      "Average Loss = 4.3408e+08:   3%|▎         | 891/30000 [00:25<14:08, 34.30it/s]\u001b[A\n",
      "Average Loss = 4.3408e+08:   3%|▎         | 895/30000 [00:26<14:32, 33.34it/s]\u001b[A\n",
      "Average Loss = 4.3408e+08:   3%|▎         | 900/30000 [00:26<13:36, 35.62it/s]\u001b[A\n",
      "Average Loss = 4.3263e+08:   3%|▎         | 900/30000 [00:26<13:36, 35.62it/s]\u001b[A\n",
      "Average Loss = 4.3263e+08:   3%|▎         | 900/30000 [00:26<13:36, 35.62it/s]\u001b[A\n",
      "Average Loss = 4.3263e+08:   3%|▎         | 904/30000 [00:26<14:36, 33.18it/s]\u001b[A\n",
      "Average Loss = 4.3263e+08:   3%|▎         | 909/30000 [00:26<14:00, 34.61it/s]\u001b[A\n",
      "Average Loss = 4.3121e+08:   3%|▎         | 909/30000 [00:26<14:00, 34.61it/s]\u001b[A\n",
      "Average Loss = 4.3121e+08:   3%|▎         | 909/30000 [00:26<14:00, 34.61it/s]\u001b[A\n",
      "Average Loss = 4.3121e+08:   3%|▎         | 913/30000 [00:26<14:46, 32.81it/s]\u001b[A\n",
      "Average Loss = 4.3121e+08:   3%|▎         | 918/30000 [00:26<13:46, 35.17it/s]\u001b[A\n",
      "Average Loss = 4.2977e+08:   3%|▎         | 918/30000 [00:26<13:46, 35.17it/s]\u001b[A\n",
      "Average Loss = 4.2977e+08:   3%|▎         | 918/30000 [00:26<13:46, 35.17it/s]\u001b[A\n",
      "Average Loss = 4.2977e+08:   3%|▎         | 922/30000 [00:26<14:53, 32.55it/s]\u001b[A\n",
      "Average Loss = 4.2977e+08:   3%|▎         | 927/30000 [00:26<13:50, 34.99it/s]\u001b[A\n",
      "Average Loss = 4.2836e+08:   3%|▎         | 927/30000 [00:27<13:50, 34.99it/s]\u001b[A\n",
      "Average Loss = 4.2836e+08:   3%|▎         | 927/30000 [00:27<13:50, 34.99it/s]\u001b[A\n",
      "Average Loss = 4.2836e+08:   3%|▎         | 931/30000 [00:27<14:47, 32.77it/s]\u001b[A\n",
      "Average Loss = 4.2836e+08:   3%|▎         | 936/30000 [00:27<13:50, 34.99it/s]\u001b[A\n",
      "Average Loss = 4.2836e+08:   3%|▎         | 940/30000 [00:27<14:37, 33.13it/s]\u001b[A\n",
      "Average Loss = 4.2693e+08:   3%|▎         | 940/30000 [00:27<14:37, 33.13it/s]\u001b[A\n",
      "Average Loss = 4.2693e+08:   3%|▎         | 940/30000 [00:27<14:37, 33.13it/s]\u001b[A\n",
      "Average Loss = 4.2693e+08:   3%|▎         | 945/30000 [00:27<13:52, 34.90it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 4.2693e+08:   3%|▎         | 949/30000 [00:27<14:59, 32.29it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "#mod1.fit_advi(n=1, method='advi')\n",
    "#mod2.fit_advi(n=1, method='advi')\n",
    "mod3.fit_advi(n=1, method='advi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<matplotlib.lines.Line2D object at 0x1468c7e01f10>]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<matplotlib.lines.Line2D object at 0x1468c9df2150>]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mod1.plot_history()\n",
    "# plt.show()\n",
    "# mod1.plot_history(20000)\n",
    "# plt.show()\n",
    "# mod2.plot_history()\n",
    "# plt.show()\n",
    "# mod2.plot_history(20000)\n",
    "# plt.show()\n",
    "mod3.plot_history()\n",
    "plt.show()\n",
    "mod3.plot_history(20000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mod1.sample_posterior(node='all', n_samples=1000, save_samples=False);\n",
    "# mod1.sample2df()\n",
    "# mod1.compute_expected()\n",
    "# mod1.plot_posterior_mu_vs_data()\n",
    "# plt.show()\n",
    "\n",
    "# mod2.sample_posterior(node='all', n_samples=1000, save_samples=False);\n",
    "# mod2.sample2df()\n",
    "# mod2.compute_expected()\n",
    "# mod2.plot_posterior_mu_vs_data()\n",
    "# plt.show()\n",
    "\n",
    "mod3.sample_posterior(node='all', n_samples=100, save_samples=False);\n",
    "mod3.sample2df()\n",
    "mod3.compute_expected()\n",
    "mod3.plot_posterior_mu_vs_data()\n",
    "mod3.plot_posterior_mu_vs_data(mu_node_name='mun', data_node='Y_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cellpymc)",
   "language": "python",
   "name": "cellpymc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
